---
layout: default
---

<script type="text/javascript">
    document.getElementById('LNcfp').id='leftcurrent';
</script>


<div class="contents">

<h1>AISTATS 2017 Accepted Papers</h1>

<i>Any typos will be corrected in the final list of proceedings. This is a temporary list in alphabetical order of the title. To address serious typos, please contact publicity chair Aaditya Ramdas at aramdas [at] berkeley.edu.</i><br><br>


A Fast and Scalable Joint Estimator for Learning Multiple Related Sparse Gaussian Graphical Models	<br>
Beilun Wang, Ji Gao, Yanjun Qi <br><br>

A Framework for Optimal Matching for Causal Inference <br>
Nathan Kallus <br><br>

A Learning Theory of Ranking Aggregation	<br>
Anna KORBA, Stéphan Clemençon, Eric Sibony <br><br>

A Lower Bound on the Partition Function of Attractive Graphical Models in the Continuous Case<br>
	Nicholas Ruozzi<br><br>

A Maximum Matching Algorithm for Basis Selection in Spectral Learning<br>
	Ariadna Quattoni,  Xavier Carreras, Matthias Gallé<br><br>


A New Class of Private Chi-Square Hypothesis Tests	<br>
Ryan Rogers, Daniel Kifer<br><br>

A Stochastic Nonconvex Splitting Method for Symmetric Nonnegative Matrix Factorization	
<br>Songtao Lu, Mingyi Hong, Zhengdao Wang <br><br>

A Sub-Quadratic Exact Medoid Algorithm	<br>
James Newling, Francois Fleuret <br><br>

A Unified Computational and Statistical Framework for Nonconvex Low-rank Matrix Estimation	<br>
Lingxiao Wang, Xiao Zhang, Quanquan Gu<br><br>

A Unified Optimization View on Generalized Matching Pursuit and Frank-Wolfe	<br>
Francesco Locatello,  Rajiv Khanna, Michael Tschannen, Martin Jaggi<br><br>

Active Positive Semidefinite Matrix Completion: Algorithms, Theory and Applications	<br>
Aniruddha  Bhargava, Ravi Ganti, Rob Nowak<br><br>

Adaptive ADMM with Spectral Penalty Parameter Selection<br>
Zheng Xu, Mario Figueiredo, Tom Goldstein <br><br>

An Information-Theoretic Route from Generalization in Expectation to Generalization in Probability	<br>
Ibrahim Alabdulmohsin<br><br>

Annular Augmentation Sampling<br>
Francois Fagan, Jalaj Bhandari, John Cunningham<br><br>

Anomaly Detection in Extreme Regions via Empirical MV-sets on the Sphere<br>
	Albert Thomas, Stéphan Clemençon, Alexandre Gramfort, Anne Sabourin<br><br>

ASAGA: Asynchronous Parallel SAGA	<br>
 Rémi Leblond, Fabian Pedregosa, Simon Lacoste-Julien<br><br>

Asymptotically exact inference in likelihood-free models<br>	
Matthew Graham, Amos Storkey<br><br>

Attributing Hacks<br>	
Ziqi Liu, Alex Smola, Kyle Soska, Yu-Xiang Wang, Qinghua Zheng <br><br>

AutoGP: Exploring the Capabilities and Limitations of Gaussian Process Models<br>
Karl Krauth, Edwin Bonilla, Kurt Cutajar, Maurizio Filippone <br><br>


Automated Inference with Adaptive Batches<br>
Soham De, Abhay Yadav, David Jacobs, Tom Goldstein <br><br>

Bayesian Hybrid Matrix Factorisation for Data Integration<br>	
Thomas Brouwer, Pietro Lio<br><br>

Belief Propagation in Conditional RBMs for Structured Prediction<br>	
Wei Ping, Alex Ihler <br><br>

Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers<br>
	Meelis Kull, Peter Flach <br><br>

Binary and Multi-Bit Coding for Stable Random Projections<br>	
Ping Li <br><br>

Black-box Importance Sampling<br>
	Qiang Liu, Jason Lee<br><br>

Clustering from Multiple Uncertain Experts<br>
	Yale Chang, Junxiang Chen,  Michael Cho,  Peter Castaldi, Ed Silverman, Jennifer Dy <br><br>

Co-Occuring Directions Sketching for Approximate Matrix Multiply<br>
	Youssef Mroueh, Etienne Marcheret, Vaibahava Goel<br><br>

Combinatorial Topic Models using Small-Variance Asymptotics<br>
	Ke Jiang, Suvrit Sra, Brian Kulis<br><br>

Communication-efficient Distributed Sparse Linear Discriminant Analysis<br>
	Lu Tian, Quanquan Gu <br><br>

Communication-Efficient Learning of Deep Networks from Decentralized Data<br>
	Brendan McMahan, Eider Moore, Daniel Ramage,  Seth Hampson,  Blaise Aguera y Arcas<br><br>


Comparison Based Nearest Neighbor Search	<br> 
Siavash Haghiri, Ulrike von Luxburg, Debarghya Ghoshdastidar<br><br>


Complementary Sum Sampling for Likelihood Approximation in Large Scale Classification<br>	
David Barber, Aleksandar Botev,  Bowen Zheng<br><br>


Compressed Least Squares Regression revisited<br>
	Martin Slawski<br><br>

Conditions beyond treewidth for tightness of higher-order LP relaxations<br>
Mark Rowland, Aldo Pacchiano,  Adrian Weller<br><br>


Conjugate-Computation Variational Inference : Converting Variational Inference in Non-Conjugate Models to Inferences in Conjugate Models<br>
	Mohammad Khan, Wu Lin<br><br>


Consistent and Efficient Nonparametric Different-Feature Selection<br>
	Satoshi Hara, Takayuki Katsuki, Hiroki Yanagisawa, Takafumi Ono, Ryo Okamoto, Shigeki Takeuchi<br><br>


Contextual Bandits with Latent Confounders: An NMF Approach<br>
	Rajat Sen, Karthikeyan Shanmugam, Murat Kocaoglu, Alex Dimakis, Sanjay Shakkottai<br><br>


Convergence rate of stochastic k-means<br>
	Cheng Tang, Claire Monteleoni<br><br>


ConvNets with Smooth Adaptive Activation Functions for Regression<br>
	Le Hou,  Dimitris Samaras,  Tahsin Kurc,  Yi Gao,  Joel Saltz<br><br>


CPSG-MCMC: Clustering-Based Preprocessing method for Stochastic Gradient MCMC<br>
	Tianfan Fu, Zhihua Zhang<br><br>


Data Driven Resource Allocation for Distributed Learning<br>
	Travis Dick, Venkata Krishna Pillutla, Mu Li, Colin White, Nina Balcan, Alex Smola<br><br>

Decentralized Collaborative Learning of Personalized Models over Networks<br>
Paul Vanhaesebrouck, Aurélien Bellet, Marc Tommasi<br><br>


Detecting Dependencies in High-Dimensional, Sparse Databases Using Probabilistic Programming and Non-parametric Bayes<br>
	Feras Saad, Vikash Mansinghka<br><br>


Discovering and Exploiting Additive Structure for Bayesian Optimization<br>
	Jacob Gardner, Chuan Guo, Kilian Weinberger, Roman Garnett, Roger Grosse<br><br>


Distance Covariance Analysis<br>
	Benjamin Cowley, Joao Semedo,  Amin Zandvakili, Adam Kohn, Matthew Smith, Byron Yu<br><br>


Distributed Sequential Sampling for Kernel Matrix Approximation<br>
	Daniele Calandriello, Alessandro Lazric, Michal Valko<br><br>


Distribution of Gaussian Process Arc Lengths<br>	
Justin Bewsher, Alessandra Tosi, Michael Osborne, Stephen Roberts<br><br>


Diversity Leads to Generalization in Neural Networks<br>
	Bo Xie, Yingyu Liang, Le Song<br><br>


DP-EM: Differentially Private Expectation Maximization<br>	
Mijung Park,  James  Foulds,  Kamalika Choudhary, Max Welling <br><br>


Dynamic Collaborative Filtering With Compound Poisson Factorization<br>	
Ghassen Jerfel, Basbug, Barbara Engelhardt <br><br>


Efficient Algorithm for Sparse Tensor-variate Gaussian Graphical Models via Gradient Descent<br>	
Pan Xu, Quanquan Gu <br><br>


Efficient Multiclass Prediction on Graphs via Surrogate Losses<br>
	Alexander Rakhlin, Karthik Sridharan<br><br>


Efficient Rank Aggregation via Lehmer Codes<br>
	Pan Li,  Arya Mazumdar, Olgica Milenkovic<br><br>


Encrypted accelerated least squares regression<br>
	Pedro Esperanca, Louis Aslett,  Chris Holmes<br><br>


Estimating Density Ridges by Direct Estimation of Density-Derivative-Ratios<br>
	Hiroaki Sasaki, Takafumi Kanamori, Masashi Sugiyama<br><br>


Exploration--Exploitation in MDPs with Options<br>	
Ronan Fruit,  Alessandro Lazric <br><br>



Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets<br>
	Aaron Klein, Stefan Falkner,  Simon Bartels, Philipp Hennig,  Frank Hutter<br><br>


Fast Classification with Binary Prototypes<br>	
Kai Zhong,  Ruiqi Guo, Sanjiv Kumar, Bowei Yan, David Simcha, Inderjit Dhillon <br><br>


Fast column generation for atomic norm regularization.<br>
	Marina Vinyes, Guillaume Obozinski <br><br>


Fast rates with high probability in exp-concave statistical learning<br>
	Nishant Mehta<br><br>


Faster Coordinate Descent via Adaptive Importance Sampling<br>
	Dmytro Perekrestenko, Volkan Cevher,  Martin Jaggi<br><br>


Finite-sum Composition Optimization via Variance Reduced Gradient Descent<br>
	Xiangru Lian, Ji Liu, Mengdi Wang <br><br>


Fokker-Planck Inference Machine: A Unifying Framework for Linking Microscopic Event History to Macroscopic Prediction<br>
	Yichen Wang, Xiaojing Ye,  Haomin Zhou, Hongyuan Zha, Le Song<br><br>


Frank-Wolfe Algorithms for Saddle Point Problems<br>	
Gauthier Gidel, Simon Lacoste-Julien,  Tony Jebara<br><br>


Frequency Domain Predictive Modelling with Aggregated Data<br>
	Avradeep Bhowmik, Joydeep Ghosh, Oluwasanmi Koyejo <br><br>


Generalization Error of Invariant Classifiers<br>
	Jure Sokolic, Raja Giryes, Guillermo Sapiro,  Miguel Rodrigues<br><br>


Generalized Pseudolikelihood Methods for Inverse Covariance Estimation<br>
	Alnur Ali, Kshitij Khare, Sang-Yun Oh, Bala Rajaratnam <br><br>


Global Convergence of Non-Convex Gradient Descent for Computing Matrix Squareroot<br>
	Prateek Jain, Chi Jin, Sham Kakade, Praneeth Netrapalli <br><br>


Gradient Boosting on Stochastic Data Streams<br>
	Hanzhang Hu, Andrew Bagnell,  Wen Sun,  Martial Hebert, Arun Venkatraman<br><br>

Gray-box inference for structured Gaussian process models<br>
	Pietro Galliani, Amir Dezfouli,  Edwin Bonilla,  Novi Quadrianto<br><br>


Greedy Direction Method of Multiplier for MAP Inference of Large Output Domain<br>
	Xiangru Huang, Ian En-Hsu Yen, Ruohan Zhang, Qixing Huang, Pradeep Ravikumar, Inderjit  Dhillon<br><br>

	
Guaranteed Non-convex Optimization: Submodular Maximization over Continuous Domains<br>
	Andrew An Bian, Baharan Mirzasoleiman, Joachim Buhmann, Andreas Krause<br><br>

Hierarchically-partitioned Gaussian Process Approximation<br>
	Byung-Jun Lee, Jongmin Lee, Kee-Eung Kim<br><br>

High-dimensional Time Series Clustering via Cross-Predictability<br>
	Dezhi Hong, Quanquan Gu, Kamin Whitehouse<br><br>

Hit-and-Run for Sampling and Planning in Non-Convex Spaces<br>
	Yasin  Abbasi-Yadkori, Alan Malek, Peter Bartlett, Victor Gabillon<br><br>

Horde of Bandits using Gaussian Markov Random Fields<br>
	Sharan Vaswani, Mark Schmidt,  Laks Lakshmanan<br><br>

Identifying groups of strongly correlated variables through Smoothed Ordered Weighted L_1-norms<br>
	Raman Sankaran,  Francis  Bach, Chiranjib Bhattacharya <br><br>

Improved Strongly Adaptive Online Learning using Coin Betting<br>
	Kwang-Sung Jun, Rebecca Willett, Stephen Wright, Francesco Orabona<br><br>

Inference Compilation and Universal Probabilistic Programming<br>
	Tuan Anh Le, Atilim Gunes Baydin, Frank Wood <br><br>

Information Projection and Approximate Inference for Structured Sparse Variables<br>
	 Rajiv Khanna, Joydeep Ghosh, Rusell Poldrack, Oluwasanmi Koyejo <br><br>

Information-theoretic limits of Bayesian network structure learning<br>
	Asish Ghoshal, Jean Honorio<br><br>

Initialization and Coordinate Optimization for Multi-way Matching<br>
	Da Tang, Tony Jebara<br><br>

Label Filters for Large Scale Multilabel Classification<br>
	Alexandru Niculescu-Mizil, Ehsan Abbasnejad <br><br>

Large-Scale Data-Dependent Kernel Approximation<br>
	Alin Popa, Catalin Ionescu, Cristian Sminchisescu <br><br>

Learning Cost-Effective Treatment Regimes using Markov Decision Processes<br>
	Himabindu Lakkaraju, Cynthia Rudin<br><br>

Learning from Conditional Distributions via Dual Kernel Embeddings<br>
	Bo Dai, Niao He, Yunpeng Pan, Byron Boots, Le Song<br><br>

Learning Graphical Games from Behavioral Data: Sufficient and Necessary Conditions<br>
	Asish Ghoshal,  Jean Honorio<br><br>


Learning Nash Equilibrium for General-Sum Markov Games from Batch Data<br>
	Julien Perolat, Florian Strub, Bilal Piot,  Olivier Pietquin<br><br>


Learning Nonparametric Forest Graphical Models with Prior Information<br>
	Yuancheng Zhu, Zhe Liu,  Siqi Sun<br><br>


Learning Optimal Interventions<br>
	Jonas Mueller,  David  Reshef, George Du,  Tommi Jaakkola<br><br>


Learning Structured Weight Uncertainty in Bayesian Neural Networks<br>
	Shengyang Sun, Changyou Chen, Lawrence Carin<br><br>


Learning the Network Structure of Heterogeneous Data via Pairwise Exponential Markov Random Fields	<br>
Youngsuk Park, David Hallac, Stephen Boyd,  Jure Leskovec<br><br>


Learning Theory for Conditional Risk Minimization<br>
	Alexander Zimin, Christoph Lampert <br><br>

Learning Time Series Detection Models from Temporally Imprecise Labels<br>
	Roy Adams, Ben Marlin<br><br>

Learning with feature feedback: from theory to practice	<br>
Stefanos Poulis, Sanjoy Dasgupta<br><br>

Least-Squares Log-Density Gradient Clustering for Riemannian Manifolds<br>
	Mina Ashizawa, Hiroaki Sasaki, Tomoya Sakai,  Masashi Sugiyama<br><br>

Less than a Single Pass: Stochastically Controlled Stochastic Gradient Method<br>
	Lihua Lei,  Michael Jordan<br><br>

Linear Convergence of Stochastic Frank Wolfe Variants<br>
	Chaoxu Zhou, Donald Goldfarb, Garud Iyengar<br><br>

Linear Thompson Sampling Revisited<br>
	Marc Abeille, Alessandro Lazric <br><br>

Lipschitz Density-Ratios, Structured Data, and Data-driven Tuning<br>
	Samory Kpotufe<br><br>

Local Group Invariant Representations via Orbit Embeddings<br>
	Anant Raj,  Abhishek  Kumar, Youssef Mroueh, Tom Fletcher, Bernhard Schoelkopf<br><br>

Local Perturb-and-MAP for Structured Prediction<br>
	Gedas Bertasius,  Lorenzo Torresani, Jianbo Shi, Qiang Liu <br><br>

Localized Lasso for High-Dimensional Regression<br>
	Makoto Yamada,  Takeuchi Koh,  Tomoharu Iwata, John Shawe-Taylor, Samuel Kaski <br><br>

Lower Bounds on Active Learning for Graphical Model Selection<br>
	Jonathan Scarlett,  Volkan Cevher<br><br>

Markov Chain Truncation for Doubly-Intractable Inference<br>
	Colin  Wei,  Iain Murray<br><br>

Minimax Approach to Variable Fidelity Data Interpolation<br>
	Alexey Zaytsev, Evgeny Burnaev <br><br>

Minimax density estimation for growing dimension<br>
	Daniel McDonald<br><br>

Minimax Gaussian Classification & Clustering<br>
	Tianyang  Li,  Xinyang Yi,  Constantine Carmanis,  Pradeep Ravikumar  <br><br>

Minimax-optimal semi-supervised regression on unknown manifolds<br>
	Amit Moscovich, Ariel Jaffe, Boaz Nadler  <br><br>

Modal-set estimation with an application to clustering<br>
	Heinrich Jiang, Samory Kpotufe<br><br>

Near-optimal Bayesian Active Learning with Correlated and Noisy Tests<br>
	Yuxin Chen, Hamed Hassani, Andreas Krause<br><br>

Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection<br>
	Lijie Chen, Jian Li,  Mingda Qiao<br><br>

Non-Count Symmetries in Boolean & Multi-Valued Probabilistic Graphical Models<br>
	Parag Singla, Ritesh Noothigattu, Ankit Anand,  Mausam<br><br>

Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach<br>
	Dohyung Park,  Anastasios Kyrillidis, Constantine Carmanis,  Sujay Sanghavi<br><br>

Nonlinear ICA of Temporally Dependent Stationary Sources<br>
	Aapo Hyvarinen, Hiroshi Morioka<br><br>

On the Hyperprior Choice for the Global Shrinkage Parameter in the Horseshoe Prior<br>
	Juho Piironen, Aki Vehtari <br><br>

On the Interpretability of Conditional Probability Estimates in the Agnostic Setting<br>
	Yihan Gao,  Aditya Parameswaran,  Jian Peng<br><br>


On the learnability of fully-connected neural networks<br>
	Yuchen Zhang,  Jason Lee,  Martin Wainwright,  Michael Jordan<br><br>


On the Troll-Trust Model for Edge Sign Prediction in Social Networks<br>
	Géraud Le Falher, Nicolo Cesa-Bianchi,  Claudio Gentile, Fabio Vitale <br><br>


Online Learning with Partial Monitoring: Optimal Convergence Rates<br>
	Joon Kwon, Vianney Perchet <br><br>


Online Nonnegative Matrix Factorization with General Divergences<br>
	Renbo Zhao, Vincent  Tan, Huan Xu<br><br>


Online Optimization of Smoothed Piecewise Constant Functions<br>
	Vincent Cohen-Addad,  Varun Kanade <br><br>


Optimal Recovery of Tensor Slices<br>
	Andrew Li, Vivek Farias<br><br>


Optimistic Planning for the Stochastic Knapsack Problem<br>
	Ciara Pike-Burke, Steffen Grunewalder<br><br>


Orthogonal Tensor Decompositions via Two-Mode Higher-Order SVD (HOSVD)<br>
	Miaoyan Wang, Yun Song<br><br>


Performance Bounds for Graphical Record Linkage<br>
	Rebecca C. Steorts, Mattew Barnes, Willie Neiswanger<br><br>


Phase Retrieval Meets Statistical Learning Theory: A Flexible Convex Relaxation<br>
	Sohail Bahmani, Justin Romberg<br><br>


Poisson intensity estimation with reproducing kernels<br>
	Seth Flaxman, Yee Whye Teh,  Dino Sejdinovic<br><br>

Prediction Performance After Learning in Gaussian Process Regression<br>
	Johan Wagberg, Dave Zachariah, Thomas Schon,  Petre Stoica <br><br>


Quantifying the accuracy of approximate diffusions and Markov chains<br>
	Jonathan Huggins, James Zou<br><br>


Random Consensus Robust PCA<br>
	Daniel Pimentel-Alarcon, Robert Nowak<br><br>


Random projection design for scalable implicit smoothing of randomly observed stochastic processes	<br>
Francois Belletti,  Evan Sparks,  Alexandre Bayen,  Kurt Keutzer, Joseph Gonzalez<br><br>


Rank Aggregation and Prediction with Item Features<br>
	Kai-Yang Chiang, Cho-Jui Hsieh, Inderjit  Dhillon <br><br>


Rapid Mixing Swendsen-Wang Sampler for Stochastic Partitioned Attractive Models	<br>
Sejun Park, Yunhun Jang, Andreas Galanis, Jinwoo Shin, Daniel Stefankovic, Eric Vigoda <br><br>


Recurrent Switching Linear Dynamical Systems<br>
	Scott Linderman,  Andrew Miller,  David Blei,  Ryan Adams,  Liam Paninski, Matthew Johnson <br><br>


Regression Uncertainty on the Grassmannian	<br>
Yi Hong, Xiao Yang, Roland Kwitt,  Martin Styner, Marc Niethammer <br><br>


Regret Bounds for Lifelong Learning<br>	
Pierre Alquier, Tien Mai,  Massimiliano Pontil<br><br>

Regret Bounds for Transfer Learning in Bayesian Optimisation<br>
	Alistair Shilton, Sunil Gupta, Santu Rana, Svetha Venkatesh<br>


Rejection Sampling Variational Inference	<br>
Christian Naesseth, Francisco Ruiz, Scott Linderman, David Blei<br><br>


Relativistic Monte Carlo <br>
	Xiaoyu Lu, Valerio Perrone, Leonard Hasenclever, Yee Whye Teh, Sebastian Vollmer<br><br>


Removing Phase Transitions from Gibbs Measures<br>
	Ian Fellows<br><br>


Robust and Efficient Computation of Eigenvectors in a Generalized Spectral Method for Constrained Clustering<br>
	Chengming Jiang, Huiqing Xie, Zhaojun Bai<br><br>


Robust Causal Estimation in the Large-Sample Limit without strong Faithfulness<br>
	Ioan Gabriel Bucur, Tom Heskes, Tom Claassen<br><br>


Scalable Convex Multiple Sequence Alignment via Entropy-Regularized Dual Decomposition<br>
	Jiong Zhang, Ian En-Hsu Yen, Pradeep Ravikumar, Inderjit  Dhillon <br><br>


Scalable Greedy Support Selection via Weak Submodularity<br>
	 Rajiv Khanna, Ethan Elenberg, Joydeep Ghosh, Alex Dimakis<br><br>

Scalable Learning of Non-Decomposable Objectives<br>
	Elad Eban, Mariano Schain, Alan Mackey, Ariel Gordon,  Ryan Rifkin,  Gal Elidan<br><br>


Scalable variational inference for super resolution microscopy<br>
	Ruoxi Sun, Evan  Archer,  Liam  Paninski<br><br>


Scaling Submodular Maximization via Pruned Submodularity Graphs<br>
	Tianyi Zhou, Hua Ouyang,  Yi Chang,  Jeff Blimes,  Carlos Guestrin<br><br>


Sequential Graph Matching with Sequential Monte Carlo<br>
	Seong-Hwan Jun, Alexandre Bouchard-Cote, Samuel W.K. Wong<br><br>


Sequential Multiple Hypothesis Testing with Type I Error Control<br>
	Alan Malek, Yinlam Chow, Mohammad Ghavamzadeh, Sumeet Katariya <br><br>


Signal-based Bayesian Seismic Monitoring<br>
	David Moore, Stuart Russell<br><br>


Sketching Meets Random Projection in the Dual: A Provable Recovery Algorithm for Big and High-dimensional Data<br>
	Jialei Wang, Jason Lee,  Mehrdad Mahdavi,  Mladen Kolar, Nati Srebro<br><br>


Sketchy Decisions: Convex Low-Rank Matrix Optimization with Optimal Storage<br>
Alp Yurtsever, Volkan Cevher, Madeleine Udell, Joel Tropp<br><br>


Sparse Accelerated Exponential Weights<br>
	Pierre Gaillard,  Olivier Wintenberger<br><br>


Sparse Randomized Partition Trees for Nearest Neighbor Search<br>
	Kaushik Sinha, Omid Keivani<br><br>


Spatial Decompositions for Large Scale SVMs<br>
	Philipp Thomann, Ingo Steinwart, Ingrid Blaschzyk,  Mona Meister<br><br>


Spectral Methods for Correlated Topic Models<br>
	Forough Arabshahi, Anima Anandkumar<br><br>


Stochastic Difference of Convex Algorithm and its Application to Training Deep Boltzmann Machines<br>
	Atsushi Nitanda, Taiji Suzuki<br><br>


Stochastic Rank-1 Bandits<br>
	Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, Claire Vernade, Zheng Wen<br><br>


Structured adaptive and random spinners for fast machine learning computations<br>
	Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Francois Fagan, Cedric Gouy-Pailler, Anne Morvan,  Nouri Sakr,  Tamas Sarlos,  Jamal Atif <br><br>

Tensor-Dictionary Learning with Deep Kruskal-Factor Analysis<br>
	Andrew Stevens, Yunchen Pu, Yannan Sun,  Gregory Spell, Lawrence Carin <br><br>

The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear Bandits<br>
	Tor  Lattimore,  Csaba Szepesvari<br><br>

Thompson Sampling for Linear-Quadratic Control Problems<br>
	Marc Abeille, Alessandro Lazric <br><br>


Tracking Objects with Higher Order Interactions via Delayed Column Generation<br>
	Shaofei Wang, Steffen Wolf, Charless Fowlkes, Julian Yarkony<br><br>


Trading off Rewards and Errors in Multi-Armed Bandits<br>
	Akram Erraqabi, Alessandro Lazric, Michal Valko, Yun-En Liu,  Emma Brunskill <br><br>

Training Fair Classifiers<br>
	Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, Krishna Gummadi<br><br>


Unsupervised Sequential Sensor Acquisition<br>
	Manjesh Hanawal, Venkatesh Saligrama, Csaba Szepesvari<br><br>


Value-Aware Loss Function for Model-based Reinforcement Learning<br>
	Amir-Massoud Farahmand, Andre Barreto, Daniel Nikovski <br><br>




<h1>Call for Papers</h1>


AISTATS is an interdisciplinary gathering of researchers at the intersection of  artificial intelligence, machine learning, statistics, and related areas. The 20th International Conference on Artificial Intelligence and Statistics (<a href="http://www.aistats.org">AISTATS</a>) will take place in Fort Lauderdale, Florida, USA from <b>April 20-22, 2017</b>. <br><br>

The deadline for paper submission is <b>Oct 13, 2016</b> at 23:59 UTC/GMT (<a href="http://www.timeanddate.com/worldclock/converter.html">time zone converter</a>), with final decisions made on Jan 24, 2017. Please use the <a href="https://cmt.research.microsoft.com/AISTATS2017/">Microsoft CMT website</a> for all submissions.<br><br>
 
<font color = blue>New this year:</font>
<ol>
<li>  <b>Fast-track for Electronic Journal of Statistics</b>:   Authors of a small number of accepted papers will be invited to submit an extended version for fast-track publication in a special issue of the Electronic Journal of Statistics (EJS) after the AISTATS decisions are out. Details on how to prepare such extended journal paper submission will be announced after the AISTATS decisions.<br><br>

<li> <b>Review-sharing with NIPS</b>:   Papers previously submitted to NIPS 2016 are required to declare their previous NIPS paper ID, and supply a one-page letter of revision (similar to a revision letter to journal editors; anonymized) in supplemental materials. We will be using duplication detection software on NIPS data to detect revised resubmitted papers that were not declared. AISTATS reviewers will have access to the previous anonymous NIPS reviews.  Other than this, all submissions will be treated equally. <br><br>
</ol>

<font color = blue>Continuing from last year:</font>
<ol>
<li><b>Requests for code</b>: Reviewers may request public or non-proprietary code (and as necessary, accompanying data) as part of the initial reviews for the purpose of better judging the paper.  The authors will then provide the code/data as part of the author response.  This might be, for instance, to check whether the authors' methods work as claimed, or whether it correctly treats particular scenarios the authors did not consider in their initial submission."<br><br>
</ol>
 
<b>Paper Submission:</b> Electronic submission of PDF papers is required. The main part of the paper (single PDF up to 5Mb) may be up to 8 double-column pages in length including tables/figures.  References only can exceed the 8 page limit. The main part should have enough information so that reviewers are able to judge the correctness and merit of the paper.  Authors may optionally submit supplementary material (up to 10Mb) as a single zip file, containing additional proofs, audio, images, video, data or source code.  Reviewing any supplementary material is up to the discretion of the reviewers.<br><br>
 
<b>Dual Submissions Policy:</b> Submissions that are identical (or substantially similar) to versions that have been previously published, or accepted for publication, or that have been submitted in parallel to other conferences or journals are not appropriate for AISTATS and violate our dual submission policy. Exceptions to this rule are the following: (a) it is acceptable to submit work that has been made available as a technical report or similar, e.g., on arXiv, without citing it (to preserve anonymity). (b) Submission is permitted for papers presented or to be presented at conferences or workshops without proceedings (e.g., ICML or NIPS workshops), or with only abstracts published. The dual-submission rules apply during the whole AISTATS review period until the authors have been notified about the decision on their paper.<br><br>
 
<b>Double-blind review:</b> Papers will be selected via a rigorous double-blind peer-review process (the reviewers will not know the identities of the authors, and vice versa). It will be up to the authors to ensure the proper anonymization of their paper and supplemental materials.  Violation of the above rules may lead to rejection without review.  One round of author rebuttal will occur with the initial reviews available to the authors.    <br><br>
 
<b>Evaluation Criteria:</b> Submissions will be judged on the basis of technical quality, novelty, potential impact, and clarity. Typical papers often (but not always) consist of a mix of algorithmic, theoretical and experimental results, in varying proportions. Results will be judged on the degree to which they have been objectively established and/or their potential for scientific and technological impact.<br><br>
 
<b>Publication and presentation:</b> All accepted papers will be presented at the conference as posters, with a few selected for additional oral presentation. All accepted papers will be treated equally when published in the AISTATS Conference Proceedings (Journal of Machine Learning Research Workshop and Conference Proceedings series).  At least one author of each accepted paper must register and attend AISTATS.  A small number of accepted papers will be invited to submit an extended version for fast-track publication in a special issue of the Electronic Journal of Statistics (EJS) journal after the AISTATS decisions are out.<br><br>
 
<b>Topics:</b> Since its inception in 1985, the primary goal of AISTATS has been to promote the exchange of ideas from artificial intelligence, machine learning, and statistics.  We encourage the submission of all papers in keeping of this objective.  Solicited topics include, but are not limited to:
 
 <ul>
<li> Supervised, unsupervised and semi-supervised learning, kernel and Bayesian methods
 
<li> Stochastic processes, hypothesis testing, causality, time-series, nonparametrics, asymptotic theory
 
<li> Graphical models and inference, manifold learning and embedding, network analysis, statistical analysis of deep learning

<li> Sparse models and compressed sensing, information theory
 
<li> Reinforcement learning, planning, control, multi-agent systems, logic and probability, relational learning
 
<li> Learning theory, game theoretic learning, online learning, bandits, learning for mechanism design
 
<li> Convex and non-convex optimization, discrete optimization, Bayesian optimization
 
<li> Algorithms and architectures for high-performance computing
 
<li> Applications in biology, cognition, computer vision, natural language, neuroscience, robotics, etc.
 
<li> Topological data analysis, selective inference, experimental design, interactive learning, optimal teaching, and other emerging topics
</ul>

<br>

<b>Formatting Instructions: </b> 

Please download the <a href="AISTATS2017AuthorKit.zip">the author kit for AISTATS 2017</a>. Please do not modify the layout given by the style
file.  
<!--The PDF papers, supplementary material and review rebuttals will need to be submitted at <a href="https://cmt.research.microsoft.com/AISTATS2017/">the Microsoft CMT system for AISTATS 2017</a>.-->

<br><br>

