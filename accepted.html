---
layout: default
title: Accepted Papers
hide: false
navigation_weight: 11
---

<script type="text/javascript">
    document.getElementById('LNcfp').id='leftcurrent';
</script>


<div class="contents">

<h1>AISTATS 2018 Accepted Papers</h1>

<p>The accepted papers can be found in the below table sorted in increasing order of Paper ID.</p>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;margin:0px auto;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-yw4l{vertical-align:top}
@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;margin: auto 0px;}}</style>
<div class="tg-wrap"><table class="tg">
  <tr>
    <th class="tg-yw4l">Paper ID</th>
    <th class="tg-yw4l">Paper Title</th>
    <th class="tg-yw4l">Author Names</th>
  </tr>
  <tr>
    <td class="tg-yw4l">2</td>
    <td class="tg-yw4l">The Geometry of Random Features</td>
    <td class="tg-yw4l">Krzysztof Choromanski, ; Mark Rowland, University of Cambridge ; Tamas Sarlos, Google Research; Vikas Sindhwani, Google Brain Robotics; Richard Turner, Cambridge; Adrian Weller*, University of Cambridge</td>
  </tr>
  <tr>
    <td class="tg-yw4l">4</td>
    <td class="tg-yw4l">Gauged Mini-Bucket Elimination for Approximate Inference</td>
    <td class="tg-yw4l">Sungsoo Ahn, KAIST; Michael Chertkov, Los Alamos National Laborator; Jinwoo Shin, KAIST; Adrian Weller*, University of Cambridge</td>
  </tr>
  <tr>
    <td class="tg-yw4l">8</td>
    <td class="tg-yw4l">A Fast Algorithm for Separated Sparsity via Perturbed Lagrangians</td>
    <td class="tg-yw4l">Aleksander Madry, MIT; Slobodan Mitrovic*, EPFL; Ludwig Schmidt, MIT</td>
  </tr>
  <tr>
    <td class="tg-yw4l">10</td>
    <td class="tg-yw4l">An Analysis of Categorical Distributional Reinforcement Learning</td>
    <td class="tg-yw4l">Mark Rowland*, University of Cambridge ; Marc Bellemare, Google Brain; Will Dabney, DeepMind; Remi Munos, DeepMind; Yee Whye Teh, Oxford and DeepMind</td>
  </tr>
  <tr>
    <td class="tg-yw4l">16</td>
    <td class="tg-yw4l">Combinatorial Preconditioners for Proximal Algorithms on Graphs</td>
    <td class="tg-yw4l">Thomas Möllenhoff*, TU Munich; Zhenzhang Ye, ; Tao Wu, ; Daniel Cremers, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">22</td>
    <td class="tg-yw4l">Growth-Optimal Portfolio Selection under CVaR Constraints</td>
    <td class="tg-yw4l">Guy Uziel*, Technion; Ran El-Yaniv, Technion</td>
  </tr>
  <tr>
    <td class="tg-yw4l">26</td>
    <td class="tg-yw4l">Accelerated Stochastic Power Iteration</td>
    <td class="tg-yw4l">Christopher De Sa, Cornell University; Bryan He, Stanford University; Ioannis Mitliagkas, Université de Montréal; Chris Re, Stanford University; Peng Xu*, Stanford University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">27</td>
    <td class="tg-yw4l">Multi-scale Nystrom Method</td>
    <td class="tg-yw4l">Woosang Lim, Georgia Tech; Rundong Du, Georgia Tech; Bo Dai, Geogia Tech; Kyomin Jung, Seoul National University; Le Song, Georgia Tech; Haesun Park*, Georgia Tech</td>
  </tr>
  <tr>
    <td class="tg-yw4l">30</td>
    <td class="tg-yw4l">Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach</td>
    <td class="tg-yw4l">Satoshi Hara*, Osaka University; Kohei Hayashi, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">32</td>
    <td class="tg-yw4l">Mixed Membership Word Embeddings for Computational Social Science</td>
    <td class="tg-yw4l">James Foulds*, UMBC</td>
  </tr>
  <tr>
    <td class="tg-yw4l">35</td>
    <td class="tg-yw4l">Fast Threshold Tests for Detecting Discrimination</td>
    <td class="tg-yw4l">Emma Pierson*, Stanford University; Sam  Corbett-Davies, Stanford University; Sharad Goel, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">40</td>
    <td class="tg-yw4l">Iterative Supervised Principal Components</td>
    <td class="tg-yw4l">Juho Piironen*, Aalto University; Aki Vehtari, Aalto</td>
  </tr>
  <tr>
    <td class="tg-yw4l">42</td>
    <td class="tg-yw4l">Iterative Spectral Method for Alternative Clustering</td>
    <td class="tg-yw4l">Chieh Wu*, Northeastern University; Stratis Ioannidis, NEU; Mario Sznaier, Northeastern University; Xiangyu Li, Northeastern University; David Kaeli, Northeastern University; Jennifer Dy, North Eastern</td>
  </tr>
  <tr>
    <td class="tg-yw4l">45</td>
    <td class="tg-yw4l">Can clustering scale sublinearly with its clusters? A variational EM acceleration of GMMs and k-means</td>
    <td class="tg-yw4l">Dennis Forster*, University of Oldenburg; Jörg Lücke, University of Oldenburg</td>
  </tr>
  <tr>
    <td class="tg-yw4l">48</td>
    <td class="tg-yw4l">Parallelised Bayesian Optimisation via Thompson Sampling</td>
    <td class="tg-yw4l">Kirthevasan Kandasamy*, ; Akshay Krishnamurthy, U-Mass Amherst; Jeff Schneider, CMU; Barnabas Poczos, Carnegie Mellon University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">49</td>
    <td class="tg-yw4l">On the challenges of learning with inference networks on sparse, high-dimensional data</td>
    <td class="tg-yw4l">Rahul Krishnan*, MIT; Dawen Liang, Netflix; Matthew Hoffman, Google; Matthew Hoffman, Google; Dawen Liang, Netflix</td>
  </tr>
  <tr>
    <td class="tg-yw4l">54</td>
    <td class="tg-yw4l">Post Selection Inference with Kernels</td>
    <td class="tg-yw4l">Makoto Yamada*, RIKEN; Yuta Umezu, ; Kenji Fukumizu, ; Ichiro Takeuchi, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">55</td>
    <td class="tg-yw4l">On how complexity effects the stability of a predictor</td>
    <td class="tg-yw4l">Joel Ratsaby*, Ariel University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">56</td>
    <td class="tg-yw4l">On the Truly Block Eigensolvers via First-Order Riemannian Optimization</td>
    <td class="tg-yw4l">Zhiqiang Xu*, KAUST; Xin Gao, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">59</td>
    <td class="tg-yw4l">Layerwise Systematic Scan: Deep Boltzmann Machines and Beyond</td>
    <td class="tg-yw4l">Heng Guo*, University of Edinburgh; Kaan Kara, ETH Zurich; Ce Zhang, ETH Zurich</td>
  </tr>
  <tr>
    <td class="tg-yw4l">60</td>
    <td class="tg-yw4l">IHT dies hard: Provable accelerated Iterative Hard Thresholding</td>
    <td class="tg-yw4l">Rajiv Khanna, UT Austin; Anastasios Kyrillidis*, IBM T.J. Watson Research Cente</td>
  </tr>
  <tr>
    <td class="tg-yw4l">65</td>
    <td class="tg-yw4l">Finding Global Optima in Nonconvex Stochastic Semidefinite Optimization with Variance Reduction</td>
    <td class="tg-yw4l">Jinshan ZENG*, Hongkong University of Science and Technology; Ke Ma, (IIE, CAS; Yuan Yao, Hongkong University of Science and Techonology</td>
  </tr>
  <tr>
    <td class="tg-yw4l">66</td>
    <td class="tg-yw4l">Outlier Detection and Robust Estimation in Nonparametric Regression</td>
    <td class="tg-yw4l">Dehan Kong, Univ. of Toronto; Howard Bondell, North Carolina State University; Weining Shen*, UC Irvine</td>
  </tr>
  <tr>
    <td class="tg-yw4l">68</td>
    <td class="tg-yw4l">Integral Transforms from Finite Data: An Application of Gaussian Process Regression to Fourier Analysis</td>
    <td class="tg-yw4l">Luca Ambrogioni*, Radboud University; Eric Maris, Radboud University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">72</td>
    <td class="tg-yw4l">AdaGeo: Adaptive Geometric Learning for Optimization and Sampling</td>
    <td class="tg-yw4l">Gabriele Abbati*, University of Oxford; Alessandra Tosi, Mind Foundry, Oxford; Seth Flaxman, Imperial College London; Michael Osborne, Oxford</td>
  </tr>
  <tr>
    <td class="tg-yw4l">74</td>
    <td class="tg-yw4l">Online Learning with Non-Convex Losses and Non-Stationary Regret</td>
    <td class="tg-yw4l">Xiaobo Li*, University of Minnesota; Xiang Gao, University of Minnesota; Shuzhong Zhang, University of Minnesota</td>
  </tr>
  <tr>
    <td class="tg-yw4l">75</td>
    <td class="tg-yw4l">Learning Determinantal Point Processes in Sublinear Time</td>
    <td class="tg-yw4l">Christophe Dupuy*, INRIA; Francis Bach, INRIA - ENS</td>
  </tr>
  <tr>
    <td class="tg-yw4l">76</td>
    <td class="tg-yw4l">Nonlinear Structured Signal Estimation in High Dimensions via Iterative Hard Thresholding</td>
    <td class="tg-yw4l">Kaiqing Zhang, University of Illinois at Urba; Zhuoran Yang*, Princeton University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">77</td>
    <td class="tg-yw4l">Riemannian stochastic quasi-Newton algorithm with variance reduction and its convergence analysis</td>
    <td class="tg-yw4l">Hiroyuki Kasai*, UEC; Hiroyuki Sato, Kyoto University; Bamdev Mishra, Amazon</td>
  </tr>
  <tr>
    <td class="tg-yw4l">78</td>
    <td class="tg-yw4l">Online Boosting Algorithms for Multi-label Ranking</td>
    <td class="tg-yw4l">Young Hun Jung*, Universith of Michigan; Ambuj Tewari, Universith of Michigan</td>
  </tr>
  <tr>
    <td class="tg-yw4l">80</td>
    <td class="tg-yw4l">Zeroth-Order Online Alternating Direction Method of Multipliers: Convergence Analysis and Applications</td>
    <td class="tg-yw4l">Sijia Liu*, University of Michigan; Jie Chen, ; Pin-Yu Chen, ;  Alfred  Hero, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">86</td>
    <td class="tg-yw4l">High-dimensional Bayesian optimization via additive models with overlapping groups</td>
    <td class="tg-yw4l">Paul Rolland*, EPFL, LIONS; Jonathan Scarlett, ; Ilija Bogunovic, ; Volkan Cevher, EPFL</td>
  </tr>
  <tr>
    <td class="tg-yw4l">89</td>
    <td class="tg-yw4l">Robust Active Label Correction</td>
    <td class="tg-yw4l">Jan Kremer, University of Copenhagen; Fei Sha, UCLA; Christian Igel*, University of Copenhagen</td>
  </tr>
  <tr>
    <td class="tg-yw4l">90</td>
    <td class="tg-yw4l">Factorial HMM with Collapsed Gibbs Sampling for optimizing long-term HIV Therapy</td>
    <td class="tg-yw4l">Amit Gruber*, IBM Research; Chen Yanover, IBM Research; Tal El-Hay, IBM Research; Yaara Goldschmidt, IBM Research; Anders Sönnerborg, Karolinska Institute, Karolinska University Hospital; Vanni Borghi, Modena University Hospital; Francesca Incardona, EuResist Network GEIE, InformaPro S.r.l.</td>
  </tr>
  <tr>
    <td class="tg-yw4l">91</td>
    <td class="tg-yw4l">Optimal Submodular Extensions for Marginal Estimation</td>
    <td class="tg-yw4l">Pankaj Pansari*, University of Oxford; Chris Russell, The Alan Turing Institute; M. Pawan Kumar, University of Oxford</td>
  </tr>
  <tr>
    <td class="tg-yw4l">92</td>
    <td class="tg-yw4l">Semi-Supervised Learning with Competitive Infection Models</td>
    <td class="tg-yw4l">Nir Rosenfeld*, Harvard University; Amir Globerson, Tel Aviv University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">94</td>
    <td class="tg-yw4l">Discriminative Learning of Prediction Intervals</td>
    <td class="tg-yw4l">Nir Rosenfeld*, Harvard University; Yishay Mansour, Tel Aviv University; Elad Yom Tov, Microsoft Research</td>
  </tr>
  <tr>
    <td class="tg-yw4l">95</td>
    <td class="tg-yw4l">Topic Compositional Neural Language Model</td>
    <td class="tg-yw4l">Wenlin Wang*, Duke University; Zhe Gan, Duke University; Wenqi Wang, Purdue University; Dinghan Shen, Duke University; Jiaji Huang, Baidu Silicon Valley Artificial Intelligence Lab; Wei Ping, Baidu Silicon Valley Artificial Intelligence Lab; Sanjeev Satheesh, Baidu Silicon Valley Artificial Intelligence Lab; Lawrence Carin, Duke</td>
  </tr>
  <tr>
    <td class="tg-yw4l">97</td>
    <td class="tg-yw4l">Learning Priors for Invariance</td>
    <td class="tg-yw4l">Eric Nalisnick*, UC Irvine; Padhraic Smyth, University of California, Irvine</td>
  </tr>
  <tr>
    <td class="tg-yw4l">98</td>
    <td class="tg-yw4l">Optimal Cooperative Inference</td>
    <td class="tg-yw4l">Scott Cheng-Hsin Yang*, Rutgers University--Newark; Yue Yu, Rutgers University--Newark; arash Givchi, Rutgers University--Newark; Pei Wang, Rutgers University--Newark; wai Keen Vong, Rutgers University--Newark; Patrick Shafto, Rutgers University--Newark</td>
  </tr>
  <tr>
    <td class="tg-yw4l">102</td>
    <td class="tg-yw4l">Stochastic Multi-armed Bandits in Constant Space</td>
    <td class="tg-yw4l">David Liau, UT-Austin; Zhao Song, UT-Austin; Eric Price, UT-Austin; Ger Yang*, UT-Austin</td>
  </tr>
  <tr>
    <td class="tg-yw4l">109</td>
    <td class="tg-yw4l">Matrix completability analysis via graph k-connectivity</td>
    <td class="tg-yw4l">Dehua Cheng*, Univ. of Southern California; Natali Ruchansky, ; Yan Liu, University of Southern California</td>
  </tr>
  <tr>
    <td class="tg-yw4l">112</td>
    <td class="tg-yw4l">FLAG n’ FLARE: Fast Linearly-Coupled Adaptive Gradient Methods</td>
    <td class="tg-yw4l">Xiang Cheng, UC Berkeley; Fred Roosta*, University of Queensland; Stefan Palombo, UC Berkeley; Peter Bartlett, UC Berkeley; Michael Mahoney, UC Berkeley</td>
  </tr>
  <tr>
    <td class="tg-yw4l">113</td>
    <td class="tg-yw4l">Multi-view Metric Learning in Vector-valued Kernel Spaces</td>
    <td class="tg-yw4l">Riikka Huusari*, Aix-Marseille Université; Hachem Kadri, Aix-Marseille University; Cécile Capponi, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">115</td>
    <td class="tg-yw4l">Gaussian Process Subset Scanning for Anomalous Pattern Detection in Non-iid Data</td>
    <td class="tg-yw4l">William Herlands*, Carnegie Mellon University; Edward McFowland, ; Andrew Wilson, Cornell University; Daniel Neill, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">117</td>
    <td class="tg-yw4l">Dropout as a Low-Rank Regularizer for Matrix Factorization</td>
    <td class="tg-yw4l">Jacopo Cavazza*, Istituto Italiano di Tecnologi; Pietro Morerio, Istituto Italiano di Tecnologia; Benjamin Haeffele, Johns Hopkins University; Connor Lane, Johns Hopkins University; Vittorio Murino, Istituto Italiano di Tecnologia; Rene Vidal, Johns Hopkins University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">119</td>
    <td class="tg-yw4l">A Simple Analysis for Exp-concave  Empirical  Minimization  with Arbitrary Convex Regularizer</td>
    <td class="tg-yw4l">Tianbao Yang*, University of Iowa; Zhe Li, ; Lijun Zhang, Nanjing University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">120</td>
    <td class="tg-yw4l">Independently Interpretable Lasso: A New Regularizer for Sparse Regression with Uncorrelated Variables</td>
    <td class="tg-yw4l">Masaaki Takada*, The Graduate University for Advanced Studies; Taiji Suzuki, The University of Tokyo; Hironori Fujisawa, The Insitute of Statistical Mathematics</td>
  </tr>
  <tr>
    <td class="tg-yw4l">121</td>
    <td class="tg-yw4l">Boosting Variational Inference: an Optimization Perspective</td>
    <td class="tg-yw4l">Francesco  Locatello*, ETH Zurich; Rajiv Khanna, UT Austin; Joydeep Ghosh, ; Gunnar Ratsch, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">122</td>
    <td class="tg-yw4l">Personalized and Private Peer-to-Peer Machine Learning</td>
    <td class="tg-yw4l">Aurélien Bellet*, INRIA; Rachid Guerraoui, ; mahsa Taziki, ; Marc Tommasi, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">125</td>
    <td class="tg-yw4l">Tensor Regression Meets Gaussian Processes</td>
    <td class="tg-yw4l">Rose Yu*, Caltech; Guangyu Li, University of Southern California; Yan Liu, University of Southern California</td>
  </tr>
  <tr>
    <td class="tg-yw4l">127</td>
    <td class="tg-yw4l">A Nonconvex Proximal Splitting Algorithm under Moreau-Yosida Regularization</td>
    <td class="tg-yw4l">Emanuel Laude*, Technical University of Munich; Tao Wu, ; Daniel Cremers, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">133</td>
    <td class="tg-yw4l">Medoids in Almost-Linear Time via Multi-Armed Bandits</td>
    <td class="tg-yw4l">Vivek Bagaria, ; Govinda Kamath, ; Martin Zhang, Stanford University; Vasilis Ntranos, ; David Tse*, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">139</td>
    <td class="tg-yw4l">Regional Multi-Armed Bandits</td>
    <td class="tg-yw4l">Zhiyang Wang, USTC; Ruida Zhou, USTC; Cong Shen*, Univ. of Sci. &amp; Tech. China </td>
  </tr>
  <tr>
    <td class="tg-yw4l">142</td>
    <td class="tg-yw4l">Nearly second-order optimality of online joint detection and estimation via one-sample update schemes</td>
    <td class="tg-yw4l">Yang Cao*, Georgia Institute of Technolog; Liyan Xie, ; Yao Xie, ; Huan Xu, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">151</td>
    <td class="tg-yw4l">Sum-Product-Quotient Networks</td>
    <td class="tg-yw4l">Or Sharir*, Hebrew University of Jerusalem; Amnon Shashua, Hebrew University of Jerusalem</td>
  </tr>
  <tr>
    <td class="tg-yw4l">154</td>
    <td class="tg-yw4l">Exploiting Strategy-Space Diversity for Batch Bayesian Optimization</td>
    <td class="tg-yw4l">Sunil Gupta*, Deakin University; Alistair Shilton, Deakin University; Santu Rana, Deakin University; Svetha Venkatesh, Deakin University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">158</td>
    <td class="tg-yw4l">Beating Monte Carlo Integration: a Nonasymptotic Study of Kernel Smoothing Methods</td>
    <td class="tg-yw4l">Stephan Clémençon*, Telecom ParisTech; François Portier, Telecom ParisTech</td>
  </tr>
  <tr>
    <td class="tg-yw4l">166</td>
    <td class="tg-yw4l">Group invariance principles for causal generative models</td>
    <td class="tg-yw4l">Michel Besserve*, ; naji Shajarisales, MPI for Intelligent Systems; Bernhard Schoelkopf, MPI for Intelligent Systems; Dominik Janzing, MPI for Intelligent Systems</td>
  </tr>
  <tr>
    <td class="tg-yw4l">167</td>
    <td class="tg-yw4l">A Provable Algorithm for Learning Interpretable Scoring Systems</td>
    <td class="tg-yw4l">Nataliya Sokolovska*, University Paris 6; Yann Chevaleyre, University Paris Dauphine; Jean-Daniel Zucker, IRD</td>
  </tr>
  <tr>
    <td class="tg-yw4l">172</td>
    <td class="tg-yw4l">Scaling up the Automatic Statistician: Scalable Structure Discovery using Gaussian Processes</td>
    <td class="tg-yw4l">Hyunjik Kim*, University of Oxford; Yee Whye Teh, Oxford</td>
  </tr>
  <tr>
    <td class="tg-yw4l">178</td>
    <td class="tg-yw4l">Efficient Bandit Combinatorial Optimization Algorithm with Zero-suppressed Binary Decision Diagrams</td>
    <td class="tg-yw4l">Shinsaku Sakaue*, NTT; Masakazu Ishihata, Hokkaido University; Shin-ichi Minato, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">181</td>
    <td class="tg-yw4l">Transfer Learning on fMRI Datasets</td>
    <td class="tg-yw4l">Hejia Zhang*, Princeton University; Po-Hsuan Chen, Princeton University; Peter Ramadge, Princeton University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">183</td>
    <td class="tg-yw4l">An Optimization Approach to Learning Falling Rule Lists</td>
    <td class="tg-yw4l">Chaofan Chen*, Duke University; Cynthia Rudin, Duke</td>
  </tr>
  <tr>
    <td class="tg-yw4l">185</td>
    <td class="tg-yw4l">Catalyst for Gradient-based Nonconvex Optimization</td>
    <td class="tg-yw4l">Courtney Paquette*, Ohio State University; Hongzhou Lin, INRIA; Dmitriy Drusvyatskiy, University of Washington; Julien Mairal, Inria; Zaid Harchaoui, University of Washington</td>
  </tr>
  <tr>
    <td class="tg-yw4l">188</td>
    <td class="tg-yw4l">Benefits from Superposed Hawkes Processes</td>
    <td class="tg-yw4l">Hongteng Xu*, Duke University; Dixin Luo, ; Xu Chen, Tsinghua University; Lawrence Carin, Duke</td>
  </tr>
  <tr>
    <td class="tg-yw4l">192</td>
    <td class="tg-yw4l">Nonparametric Preference Completion</td>
    <td class="tg-yw4l">Julian Katz-Samuels*, University of Michigan; Clayton Scott, University of Michigan</td>
  </tr>
  <tr>
    <td class="tg-yw4l">198</td>
    <td class="tg-yw4l">Non-parametric estimation of Jensen-Shannon Divergence in Generative Adversarial Network training</td>
    <td class="tg-yw4l">Mathieu Sinn*, ; Ambrish Rawat, IBM Research</td>
  </tr>
  <tr>
    <td class="tg-yw4l">201</td>
    <td class="tg-yw4l">Efficient and principled score estimation with Nyström kernel exponential families</td>
    <td class="tg-yw4l">Dougal Sutherland*, Gatsby unit, UCL; Heiko Strathmann, ; Michael Arbel, Gatsby unit, UCL; Arthur Gretton, Gatsby unit, UCL</td>
  </tr>
  <tr>
    <td class="tg-yw4l">208</td>
    <td class="tg-yw4l">Symmetric Variational Autoencoder and Connections to Adversarial Learning</td>
    <td class="tg-yw4l">Liqun Chen*, Duke University; Shuyang Dai, Duke University; Yunchen Pu, Duke University; Chunyuan Li, Duke University; Qinliang Su, Duke University; Erjin Zhou, Face++; Lawrence Carin, Duke</td>
  </tr>
  <tr>
    <td class="tg-yw4l">210</td>
    <td class="tg-yw4l">Few-shot Generative Modelling with Generative Matching Networks</td>
    <td class="tg-yw4l">Sergey Bartunov*, DeepMind; Dmitry Vetrov, Higher School of Economics</td>
  </tr>
  <tr>
    <td class="tg-yw4l">211</td>
    <td class="tg-yw4l">Nonlinear Weighted Finite Automata</td>
    <td class="tg-yw4l">Tianyu Li*, McGill University; Guillaume Rabusseau, McGill University; Doina Precup, McGill University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">212</td>
    <td class="tg-yw4l">Natural Gradients in Practice: Non-Conjugate Variational Inference in Gaussian Process Models</td>
    <td class="tg-yw4l">Hugh Salimbeni*, Imperial College London; Stefanos Eleftheriadis, Prowler.io; James Hensman, PROWLER.io</td>
  </tr>
  <tr>
    <td class="tg-yw4l">216</td>
    <td class="tg-yw4l">Variational inference for the multi-armed contextual bandit</td>
    <td class="tg-yw4l">Iñigo Urteaga*, Columbia University; Chris Wiggins, Columbia University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">220</td>
    <td class="tg-yw4l">Tracking the gradients using the Hessian: A new look at variance reducing stochastic methods</td>
    <td class="tg-yw4l">Robert Gower*, Telecom Paristech; Nicolas Le Roux, Google Brain; Francis Bach, Inria / ENS</td>
  </tr>
  <tr>
    <td class="tg-yw4l">226</td>
    <td class="tg-yw4l">Subsampling for Ridge Regression via Regularized Volume Sampling</td>
    <td class="tg-yw4l">Michal Derezinski*, UC Santa Cruz; Manfred Warmuth, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">228</td>
    <td class="tg-yw4l">Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition</td>
    <td class="tg-yw4l">Pavel Izmailov*, Cornell University; Dmitry Kropotov, MSU; Alexander Novikov, Higher school of economics</td>
  </tr>
  <tr>
    <td class="tg-yw4l">229</td>
    <td class="tg-yw4l">Batch-Expansion Training: An Efficient Optimization Framework</td>
    <td class="tg-yw4l">Michal Derezinski*, UC Santa Cruz; Dhruv Mahajan, Facebook Research; S. Sathiya Keerthi, Microsoft Corporation; S. V. N. Vishwanathan, UC Santa Cruz; Markus Weimer, Microsoft Corporation</td>
  </tr>
  <tr>
    <td class="tg-yw4l">237</td>
    <td class="tg-yw4l">Batched Large-scale Bayesian Optimization in High-dimensional Spaces</td>
    <td class="tg-yw4l">Zi Wang*, MIT; Clement Gehring, ; Stefanie Jegelka, MIT; Pushmeet Kohli, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">244</td>
    <td class="tg-yw4l">A Bayesian Nonparametric Method for Clustering Imputation, and Forecasting in Multivariate Time Series</td>
    <td class="tg-yw4l">FERAS SAAD*, MIT; Vikash Mansinghka, MIT</td>
  </tr>
  <tr>
    <td class="tg-yw4l">245</td>
    <td class="tg-yw4l">Stochastic Three-Composite Convex Minimization with a Linear Operator</td>
    <td class="tg-yw4l">Renbo Zhao*, NUS; Volkan Cevher, EPFL</td>
  </tr>
  <tr>
    <td class="tg-yw4l">246</td>
    <td class="tg-yw4l">Direct Learning to Rank And Rerank</td>
    <td class="tg-yw4l">Cynthia Rudin*, Duke; Yining Wang, Carnegie Mellon University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">247</td>
    <td class="tg-yw4l">One-shot Coresets: The Case of k-Clustering</td>
    <td class="tg-yw4l">Olivier Bachem*, ETH Zurich; Mario Lucic, Google Brain Zurich; Silvio Lattanzi, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">249</td>
    <td class="tg-yw4l">Random Warping Series: A Random Features Method for Time-Series Embedding</td>
    <td class="tg-yw4l">Lingfei Wu*, IBM T. J. Watson Research Cent; Ian En-Hsu Yen, CMU; Jinfeng Yi, ; Fangli Xu, College of William and Mary; Qi Lei, University of Texas at Austin; Michael Witbrock, IBM T. J. Watson Research Center</td>
  </tr>
  <tr>
    <td class="tg-yw4l">250</td>
    <td class="tg-yw4l">Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD</td>
    <td class="tg-yw4l">Sanghamitra Dutta*, Carnegie Mellon University; Gauri Joshi, Carnegie Mellon University; Soumyadip Ghosh, IBM Research; Parijat Dube, IBM Research; Priya Nagpurkar, IBM Research</td>
  </tr>
  <tr>
    <td class="tg-yw4l">251</td>
    <td class="tg-yw4l">Variational Inference based on Robust Divergences</td>
    <td class="tg-yw4l">Futoshi Futami*, The University of Tokyo/RIKEN; Issei Sato, The University of Tokyo / RIKEN; Masashi Sugiyama, RIKEN / The University of Tokyo</td>
  </tr>
  <tr>
    <td class="tg-yw4l">255</td>
    <td class="tg-yw4l">Resampled Proposal Distributions for Variational Inference and Learning</td>
    <td class="tg-yw4l">Aditya Grover*, Stanford University; Ramki Gummadi, ; Miguel Lazaro-Gredilla, Vicarious; Dale Schuurmans, ; Stefano Ermon, Stanford</td>
  </tr>
  <tr>
    <td class="tg-yw4l">257</td>
    <td class="tg-yw4l">Best arm identification in multi-armed bandits with delayed and partial feedback</td>
    <td class="tg-yw4l">Aditya Grover*, Stanford University; Todor Markov, ; Stefano Ermon, Stanford</td>
  </tr>
  <tr>
    <td class="tg-yw4l">267</td>
    <td class="tg-yw4l">Fully adaptive algorithm for pure exploration in linear bandits</td>
    <td class="tg-yw4l">Liyuan Xu*, The University of Tokyo / RIKEN; Junya Honda, University of Tokyo / RIKEN; Masashi Sugiyama, RIKEN / The University of Tokyo</td>
  </tr>
  <tr>
    <td class="tg-yw4l">272</td>
    <td class="tg-yw4l">Contextual Bandits with Stochastic Experts</td>
    <td class="tg-yw4l">Rajat Sen*, University of Texas at Austin; Karthikeyan Shanmugam, IBM; Sanjay Shakkottai, University of Texas at Austin</td>
  </tr>
  <tr>
    <td class="tg-yw4l">277</td>
    <td class="tg-yw4l">Human Interaction with Recommendation Systems</td>
    <td class="tg-yw4l">Sven Schmit*, Stanford University; Carlos Riquelme, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">281</td>
    <td class="tg-yw4l">Community Detection in Hypergraphs: Optimal Statistical Limit and Efficient Algorithms</td>
    <td class="tg-yw4l">I Chien, UIUC; Chung-Yi Lin*, National Taiwan University; I-Hsiang Wang, National Taiwan University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">294</td>
    <td class="tg-yw4l">Smooth and Sparse Optimal Transport</td>
    <td class="tg-yw4l">Mathieu Blondel*, NTT; Vivien Seguy, Kyoto University; Antoine Rolet, Kyoto University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">296</td>
    <td class="tg-yw4l">Robust Maximization of Non-Submodular Objectives</td>
    <td class="tg-yw4l">Ilija  Bogunovic*, ; Junyao Zhao, ETH Zürich; Volkan Cevher, EPFL</td>
  </tr>
  <tr>
    <td class="tg-yw4l">298</td>
    <td class="tg-yw4l">Cause-Effect Inference by Comparing Regression Errors</td>
    <td class="tg-yw4l">Patrick Bloebaum*, Osaka University; Dominik Janzing, MPI for Intelligent Systems; Takashi Washio, ; Shohei Shimizu, ; Bernhard Schoelkopf, MPI for Intelligent Systems</td>
  </tr>
  <tr>
    <td class="tg-yw4l">299</td>
    <td class="tg-yw4l">Tree-based Bayesian Mixture Model for Competing Risks</td>
    <td class="tg-yw4l">Alexis Bellot*, University of Oxford; Mihaela Van der Schaar, University of Oxford</td>
  </tr>
  <tr>
    <td class="tg-yw4l">301</td>
    <td class="tg-yw4l">Actor-Critic Fictitious Play in Simultaneous Move Multistage Games</td>
    <td class="tg-yw4l">Julien Perolat*, DeepMind; Bilal Piot, DeepMind; Olivier Pietquin, DeepMind</td>
  </tr>
  <tr>
    <td class="tg-yw4l">307</td>
    <td class="tg-yw4l">Random Subspace with Trees for Feature Selection Under Memory Constraints</td>
    <td class="tg-yw4l">Antonio Sutera*, ULiège; Célia Châtel, Aix-Marseille University; Gilles Louppe, ULiège; Louis Wehenkel, ; Pierre Geurts, ULiège</td>
  </tr>
  <tr>
    <td class="tg-yw4l">308</td>
    <td class="tg-yw4l">Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information</td>
    <td class="tg-yw4l">Jakob Runge*, German Aerospace Agency</td>
  </tr>
  <tr>
    <td class="tg-yw4l">310</td>
    <td class="tg-yw4l">Quotient Normalized Maximum Likelihood Criterion for Learning Bayesian Network Structures</td>
    <td class="tg-yw4l">Tomi Silander*, Naverlabs Europe; Janne Leppä-aho, ; Elias Jääsaari, ; Teemu Roos, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">311</td>
    <td class="tg-yw4l">Convex optimization over intersection of simple sets: improved convergence rate guarantees via exact penalty approach</td>
    <td class="tg-yw4l">Achintya Kundu*, Indian Institute of Science; Francis Bach, Inria / ENS; Chiranjib Bhattacharya, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">317</td>
    <td class="tg-yw4l">Variational Sequential Monte Carlo</td>
    <td class="tg-yw4l">Christian  Naesseth*, Linköping University; Scott Linderman, ; Rajesh Ranganath, Princeton; David Blei, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">321</td>
    <td class="tg-yw4l">Statistically Efficient Estimation for Non-Smooth Probability Densities</td>
    <td class="tg-yw4l">Masaaki Imaizumi*, ISM / RIKEN; Takanori Maehara, ; Yuichi Yoshida, National Institute of Informatics</td>
  </tr>
  <tr>
    <td class="tg-yw4l">324</td>
    <td class="tg-yw4l">SDCA-Powered Inexact Dual Augmented Lagrangian Method for Fast CRF Learning</td>
    <td class="tg-yw4l">Xu Hu*, ENPC; Guillaume Obozinski, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">325</td>
    <td class="tg-yw4l">Generalized Concomitant Multi-Task Lasso for sparse multimodal regression</td>
    <td class="tg-yw4l">Mathurin Massias*, INRIA Saclay; Olivier Fercoq, LTCI - Télécom ParisTech - Université Paris Saclay; Alexandre Gramfort, INRIA Saclay; Joseph Salmon, LTCI - Télécom ParisTech - Université Paris Saclay</td>
  </tr>
  <tr>
    <td class="tg-yw4l">328</td>
    <td class="tg-yw4l">Gradient Layer: Enhancing the Convergence of Adversarial Training for Generative Models</td>
    <td class="tg-yw4l">Atsushi Nitanda*, The University of Tokyo; Taiji Suzuki, The University of Tokyo</td>
  </tr>
  <tr>
    <td class="tg-yw4l">332</td>
    <td class="tg-yw4l">Statistical Sparse Online Regression: A Diffusion Approximation Perspective</td>
    <td class="tg-yw4l">Junchi Li*, Princeton University; Qiang Sun, Princeton University; Jianqing Fan, Princeton University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">338</td>
    <td class="tg-yw4l">Guaranteed Sufficient Decrease for Stochastic Variance Reduced Gradient Optimization</td>
    <td class="tg-yw4l">Fanhua Shang*, The Chinese University of Hong Kong; Yuanyuan Liu, The Chinese University of Hong Kong; Kaiwen Zhou, The Chinese University of Hong Kong; James Cheng, The Chinese University of Hong Kong; Kelvin Kai Wing Ng, The Chinese University of Hong Kong; Yuichi Yoshida, National Institute of Informatics</td>
  </tr>
  <tr>
    <td class="tg-yw4l">340</td>
    <td class="tg-yw4l">Delayed Sampling and Automatic Rao-Blackwellization of Probabilistic Programs</td>
    <td class="tg-yw4l">Lawrence Murray*, Uppsala University; Daniel Lundén, ; Jan Kudlicka, ; David Broman, ; Thomas Schön, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">342</td>
    <td class="tg-yw4l">Learning to Round for Discrete Labeling Problems</td>
    <td class="tg-yw4l">Pritish Mohapatra*, IIIT, Hyderabad; Jawahar C.V., IIIT Hyderabad; M. Pawan Kumar, University of Oxford</td>
  </tr>
  <tr>
    <td class="tg-yw4l">350</td>
    <td class="tg-yw4l">Approximate ranking from pairwise comparisons</td>
    <td class="tg-yw4l">Reinhard Heckel*, Rice University; Max Simchowitz, UC Berkeley; Kannan Ramchandran, UC Berkeley; Martin Wainwright, UC Berkeley</td>
  </tr>
  <tr>
    <td class="tg-yw4l">352</td>
    <td class="tg-yw4l">Semi-Supervised Prediction-Constrained Topic Models</td>
    <td class="tg-yw4l">Michael Hughes*, Harvard University; John Hope, University of California, Irvine; Leah Weiner, Brown University; Thomas McCoy, Massachusetts General Hospital; Roy Perlis, Massachusetts General Hospital; Erik Sudderth, University of California, Irvine; Finale Doshi-Velez, Harvard</td>
  </tr>
  <tr>
    <td class="tg-yw4l">354</td>
    <td class="tg-yw4l">A Stochastic Differential Equation Framework for Guiding Online User Activities in Closed Loop</td>
    <td class="tg-yw4l">Yichen Wang*, Gatech; Evangelos Theodorou, ; Le Song, Georgia Tech</td>
  </tr>
  <tr>
    <td class="tg-yw4l">358</td>
    <td class="tg-yw4l">Accelerated Stochastic Mirror Descent: From Continuous-time Dynamics to Discrete-time Algorithms</td>
    <td class="tg-yw4l">Pan Xu*, University of Virginia; Tianhao Wang, ; Quanquan Gu, University of Virginia</td>
  </tr>
  <tr>
    <td class="tg-yw4l">367</td>
    <td class="tg-yw4l">A Unified Framework for Nonconvex Low-Rank plus Sparse Matrix Recovery</td>
    <td class="tg-yw4l">Xiao Zhang*, University of Virginia; Lingxiao Wang, University of Virginia; Quanquan Gu, University of Virginia</td>
  </tr>
  <tr>
    <td class="tg-yw4l">370</td>
    <td class="tg-yw4l">Bayesian Nonparametric Poisson-Process Allocation for Time-Sequence Modeling</td>
    <td class="tg-yw4l">Hongyi Ding*, The University of Tokyo; Mohammad Khan, ; Issei Sato, The University of Tokyo / RIKEN; Masashi Sugiyama, RIKEN / The University of Tokyo</td>
  </tr>
  <tr>
    <td class="tg-yw4l">371</td>
    <td class="tg-yw4l">Factor Analysis on a Graph</td>
    <td class="tg-yw4l">Masayuki Karasuyama*, ; Hiroshi  Mamitsuka, Kyoto University / Aalto University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">375</td>
    <td class="tg-yw4l">Crowdclustering with Partition Labels</td>
    <td class="tg-yw4l">Junxiang Chen*, Northeastern University; Yale Chang, Northeastern University; Peter Castaldi, Brigham and Women’s Hospital; Michael Cho, Brigham and Women’s Hospital; Brian Hobbs, Brigham and Women’s Hospital; Jennifer Dy, North Eastern</td>
  </tr>
  <tr>
    <td class="tg-yw4l">378</td>
    <td class="tg-yw4l">Learning Structural Weight Uncertainty with Stein Gradient Flows</td>
    <td class="tg-yw4l">Ruiyi Zhang, Duke University; Chunyuan Li*, Duke University; Changyou Chen, SUNY Buffalo; Lawrence Carin, Duke</td>
  </tr>
  <tr>
    <td class="tg-yw4l">382</td>
    <td class="tg-yw4l">Towards Memory-Friendly Deterministic Incremental Gradient Method</td>
    <td class="tg-yw4l">Jiahao Xie*, Zhejiang University; Hui Qian, Zhejiang University; Zebang Shen, Zhejiang University; Chao Zhang, Zhejiang University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">383</td>
    <td class="tg-yw4l">Alpha-expansion is Exact on Stable Instances</td>
    <td class="tg-yw4l">Hunter Lang*, MIT; David Sontag, MIT; Aravindan Vijayaraghavan, Northwestern University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">384</td>
    <td class="tg-yw4l">Bayesian Approaches to Distribution Regression</td>
    <td class="tg-yw4l">Ho Chung Leon Law*, University Of Oxford; Dougal Sutherland, Gatsby unit, UCL; Dino Sejdinovic, University of Oxford; Seth Flaxman, Imperial College London</td>
  </tr>
  <tr>
    <td class="tg-yw4l">386</td>
    <td class="tg-yw4l">Submodularity on Hypergraphs: From Sets to Sequences</td>
    <td class="tg-yw4l">Marko Mitrovic*, Yale University; Moran Feldman, Open University of Israel; Andreas Krause, ETH Zurich; Amin Karbasi, Yale</td>
  </tr>
  <tr>
    <td class="tg-yw4l">389</td>
    <td class="tg-yw4l">Provable Estimation of the Number of Blocks in Block Models</td>
    <td class="tg-yw4l">BOWEI YAN*, UNIVERSITY OF TEXAS AT AUSTIN; Purnamrita Sarkar, University of Texas at Austin; Xiuyuan Cheng, Duke University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">391</td>
    <td class="tg-yw4l">Differentially Private Regression with Gaussian Processes</td>
    <td class="tg-yw4l">Michael Smith*, University of Sheffield; Mauricio Álvarez, University of Sheffield; Max Zwiessele, University of Sheffield; Neil Lawrence, University of Sheffield</td>
  </tr>
  <tr>
    <td class="tg-yw4l">394</td>
    <td class="tg-yw4l">Adaptive balancing of gradient and update computation times using global geometry and approximate subproblems</td>
    <td class="tg-yw4l">Sai Praneeth Reddy Karimireddy, EPFL; Sebastian Stich, EPFL; Martin Jaggi*, EPFL</td>
  </tr>
  <tr>
    <td class="tg-yw4l">407</td>
    <td class="tg-yw4l">VAE with a VampPrior</td>
    <td class="tg-yw4l">Jakub Tomczak*, University of Amsterdam; Max Welling, University of Amsterdam</td>
  </tr>
  <tr>
    <td class="tg-yw4l">408</td>
    <td class="tg-yw4l">Structured Factored Inference for Probabilistic Programming</td>
    <td class="tg-yw4l">Avi Pfeffer, Charles River Analytics; Brian Ruttenberg, Charles River Analytics; William Kretschmer, MIT; Alison OConnor*, Charles River Analytics</td>
  </tr>
  <tr>
    <td class="tg-yw4l">410</td>
    <td class="tg-yw4l">A Generic Approach for Escaping Saddle points</td>
    <td class="tg-yw4l">Sashank Reddi, Google; Manzil Zaheer*, Carnegie Mellon University; Suvrit Sra, MIT; Barnabas Poczos, Carnegie Mellon University; Francis Bach, Inria / ENS; Ruslan Salakhutdinov, Carnegie Mellon University; Alex Smola, Amazon</td>
  </tr>
  <tr>
    <td class="tg-yw4l">411</td>
    <td class="tg-yw4l">Policy Evaluation and Optimization with Continuous Treatments</td>
    <td class="tg-yw4l">Nathan Kallus*, ; Angela Zhou, Cornell ORIE</td>
  </tr>
  <tr>
    <td class="tg-yw4l">412</td>
    <td class="tg-yw4l">Multiphase MCMC Sampling for Parameter Inference in Nonlinear Ordinary Differential Equations</td>
    <td class="tg-yw4l">Alan Lazarus*, University of Glagsow; Dirk Husmeier, Glasgow; Theodore Papamarkou, Mathematics &amp; Statistics, University of Glasgow</td>
  </tr>
  <tr>
    <td class="tg-yw4l">414</td>
    <td class="tg-yw4l">Why adaptively collected data have negative bias and how to correct for it.</td>
    <td class="tg-yw4l">Xinkun Nie*, Stanford University; Xiaoying Tian, Stanford University; Jonathan Taylor, Stanford University; James Zou, Stanford University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">425</td>
    <td class="tg-yw4l">Sparse Linear Isotonic Models</td>
    <td class="tg-yw4l">Sheng Chen*, University of Minnesota; Arindam Banerjee, University of Minnesota</td>
  </tr>
  <tr>
    <td class="tg-yw4l">431</td>
    <td class="tg-yw4l">Robustness of classifiers to uniform \ell_p and Gaussian noise</td>
    <td class="tg-yw4l">Jean-Yves Franceschi, Ecole Normale Supérieure Lyon; Alhussein Fawzi*, UCLA; Omar Fawzi, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">436</td>
    <td class="tg-yw4l">Nested CRP with Hawkes-Gaussian Processes</td>
    <td class="tg-yw4l">Xi Tan*, Purdue University; Vinayak Rao, Purdue; Jennifer Neville, Purdue University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">441</td>
    <td class="tg-yw4l">Sketching for Kronecker Product Regression and P-splines</td>
    <td class="tg-yw4l">Huaian Diao, Northeast Normal University ; Zhao Song, UT-Austin; Wen Sun*, Carnegie Mellon University; David Woodruff, Carnegie Mellon University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">442</td>
    <td class="tg-yw4l">Multimodal Prediction and Personalization of Photo Edits with Deep Generative Models</td>
    <td class="tg-yw4l">Ardavan Saeedi*, ; Matthew Hoffman, Google; Matthew Hoffman, Google; Stephen  DiVerdi, Adobe; Asma Ghandeharioun, MIT; Matthew Johnson, Google Brain; Ryan  Adams, Princeton</td>
  </tr>
  <tr>
    <td class="tg-yw4l">444</td>
    <td class="tg-yw4l">Cheap Checking for Cloud Computing: Statistical Analysis via Annotated Data Streams</td>
    <td class="tg-yw4l">Chris Hickey*, University of Warwick; Graham Cormode, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">447</td>
    <td class="tg-yw4l">Reconstruction Risk of Convolutional Sparse Dictionary Learning</td>
    <td class="tg-yw4l">Shashank Singh*, ; Barnabas Poczos, Carnegie Mellon University; Jian Ma, Carnegie Mellon University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">448</td>
    <td class="tg-yw4l">Kernel Conditional Exponential Family</td>
    <td class="tg-yw4l">Michael Arbel*, Gatsby unit, UCL; Arthur Gretton, Gatsby unit, UCL</td>
  </tr>
  <tr>
    <td class="tg-yw4l">451</td>
    <td class="tg-yw4l">Linear Stochastic Approximation: Constant Step-Size and Iterate Averaging</td>
    <td class="tg-yw4l">Chandrashekar Lakshmi-Narayanan*, Indian Institute of Science; Csaba Szepesvari, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">452</td>
    <td class="tg-yw4l">Stochastic Zeroth-order Optimization in High Dimensions</td>
    <td class="tg-yw4l">Yining Wang*, Carnegie Mellon University; Simon Du, ; Sivaraman Balakrishnan, Carnegie Mellon University; Aarti Singh, Carnegie Mellon University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">459</td>
    <td class="tg-yw4l">Teacher Improves Learning by Selecting a Training Subset</td>
    <td class="tg-yw4l">Philippe Rigollet, Massachusetts Institute of Technology; Robert Nowak, ; Xiaojin Zhu*, University of Wisconsin-Madison; Xuezhou Zhang, University of Wisconsin-Madison; Yuzhe Ma, Univ. of Wisconsin-Madison</td>
  </tr>
  <tr>
    <td class="tg-yw4l">460</td>
    <td class="tg-yw4l">Communication-Avoiding Optimization Methods for Massive-Scale Graphical Model Structure Learning</td>
    <td class="tg-yw4l">Penporn Koanantakool*, UC Berkeley; Alnur Ali, Carnegie Mellon University; Ariful Azad, Lawrence Berkeley National Laboratory; Aydin Buluc, Lawrence Berkeley National Laboratory; Dmitriy Morozov, Lawrence Berkeley National Laboratory; Sang-Yun Oh, University of California, Santa Barbara; Leonid Oliker, Lawrence Berkeley National Laboratory; Katherine Yelick, Lawrence Berkeley National Laboratory</td>
  </tr>
  <tr>
    <td class="tg-yw4l">462</td>
    <td class="tg-yw4l">Robust Vertex Enumeration for Convex Hulls in High Dimensions</td>
    <td class="tg-yw4l">Pranjal Awasthi*, Rutgers University; Bahman Kalantari, ; Yikai Zhang, Rutgers</td>
  </tr>
  <tr>
    <td class="tg-yw4l">468</td>
    <td class="tg-yw4l">Fast generalization error bound of deep learning from a kernel perspective</td>
    <td class="tg-yw4l">Taiji Suzuki*, The University of Tokyo</td>
  </tr>
  <tr>
    <td class="tg-yw4l">471</td>
    <td class="tg-yw4l">Product Kernel Interpolation for Scalable Gaussian Processes</td>
    <td class="tg-yw4l">Jacob Gardner*, Cornell University; Geoff Pleiss, Cornell University; Ruihan Wu, Tsinghua University; Kilian Weinberger, Cornell University; Andrew Wilson, Cornell University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">472</td>
    <td class="tg-yw4l">Towards Provable Learning of Polynomial Neural Networks Using Low-Rank Matrix Estimation</td>
    <td class="tg-yw4l">MOHAMMADREZA SOLTANI*, Iowa State University; Chinmay Hegde, Iowa State University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">474</td>
    <td class="tg-yw4l">Scalable Generalized Dynamic Topic Models</td>
    <td class="tg-yw4l">Patrick Jähnichen*, Humboldt-Universität zu Berlin; Florian Wenzel, Humboldt-Universität zu Berlin; Marius Kloft, Humboldt-Universität zu Berlin; Stephan Mandt, Disney Research</td>
  </tr>
  <tr>
    <td class="tg-yw4l">478</td>
    <td class="tg-yw4l">Bayesian Structure Learning for Dynamic Brain Connectivity</td>
    <td class="tg-yw4l">Michael Andersen*, Aalto University; Oluwasanmi Koyejo, UIUC; Ole Winther, DTU; Lars Kai Hansen, Technical University of Denmark; Russell Poldrack, Stanford University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">482</td>
    <td class="tg-yw4l">Large Scale Empirical Risk Minimization via Truncated Adaptive Newton Method</td>
    <td class="tg-yw4l">Mark Eisen*, University of Pennsylvania; Aryan Mokhtari, University of California, Berkeley; Alejandro Ribeiro, University of Pennsylvania</td>
  </tr>
  <tr>
    <td class="tg-yw4l">483</td>
    <td class="tg-yw4l">Frank-Wolfe Splitting via Augmented Lagrangian Method</td>
    <td class="tg-yw4l">Gauthier Gidel*, MILA; Fabian Pedregosa, UC Berkeley; Simon Lacoste-Julien, Montreal</td>
  </tr>
  <tr>
    <td class="tg-yw4l">487</td>
    <td class="tg-yw4l">Learning linear structural equation models  in polynomial time and sample complexity</td>
    <td class="tg-yw4l">Asish Ghoshal*, Purdue University; Jean Honorio, Purdue</td>
  </tr>
  <tr>
    <td class="tg-yw4l">490</td>
    <td class="tg-yw4l">Convergence diagnostics for stochastic gradient descent</td>
    <td class="tg-yw4l">Jerry  Chee*, University of Chicago; Panos Toulis, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">496</td>
    <td class="tg-yw4l">Learning Sparse Polymatrix Games in Polynomial Time and Sample Complexity</td>
    <td class="tg-yw4l">Asish Ghoshal*, Purdue University; Jean Honorio, Purdue</td>
  </tr>
  <tr>
    <td class="tg-yw4l">499</td>
    <td class="tg-yw4l">Nonparametric Sharpe Ratio Function Estimation in Heteroscedastic Regression Models via Convex Optimization</td>
    <td class="tg-yw4l">Seung-Jean Kim, ; Johan Lim, Seoul National University; Joong-Ho Won*, Seoul National University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">500</td>
    <td class="tg-yw4l">Stochastic algorithms for entropy-regularized optimal transport problems</td>
    <td class="tg-yw4l">Brahim Khalil Abid*, Ecole polytechnique; Robert Gower, Telecom Paristech</td>
  </tr>
  <tr>
    <td class="tg-yw4l">502</td>
    <td class="tg-yw4l">Plug-in Estimators for Conditional Expectations and Probabilities</td>
    <td class="tg-yw4l">Steffen Grunewalder*, Lancaster University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">503</td>
    <td class="tg-yw4l">Factorized Recurrent Neural Architectures for Longer Range Dependence</td>
    <td class="tg-yw4l">Francois Belletti*, UC Berkeley; Alex Beutel, Google Inc.; Sagar Jain, Google Inc.; Ed Chi, Google Inc.</td>
  </tr>
  <tr>
    <td class="tg-yw4l">504</td>
    <td class="tg-yw4l">On the Statistical Efficiency of Compositional Nonparametric Prediction</td>
    <td class="tg-yw4l">Yixi Xu*, Purdue University; Jean Honorio, Purdue; Xiao Wang, Purdue University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">509</td>
    <td class="tg-yw4l">Metrics for Deep Generative Models</td>
    <td class="tg-yw4l">Nutan Chen*, Volkswagen Group; Richard Kurle, ; Alexej Klushyn, ; Justin Bayer, ; Xueyan Jiang, ; Patrick van der Smagt, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">510</td>
    <td class="tg-yw4l">Combinatorial Penalties: Which structures are preserved by convex relaxations?</td>
    <td class="tg-yw4l">Marwa El Halabi*, EPFL; Francis Bach, Inria / ENS; Volkan Cevher, EPFL</td>
  </tr>
  <tr>
    <td class="tg-yw4l">513</td>
    <td class="tg-yw4l">Generalized Binary Search For Split-Neighborly Problems</td>
    <td class="tg-yw4l">Stephen Mussmann*, Stanford University; Percy Liang, Stanford University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">518</td>
    <td class="tg-yw4l">Intersection-Validation: A Method for Evaluating Structure Learning without Ground Truth</td>
    <td class="tg-yw4l">Jussi Viinikka*, ; Ralf Eggeling, University of Helsinki; Mikko Koivisto, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">522</td>
    <td class="tg-yw4l">On Statistical Optimality of Variational Bayes</td>
    <td class="tg-yw4l">Anirban Bhattacharya, Texas A&amp;M University; Debdeep Pati*, Texas A&amp;M University; Yun Yang, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">524</td>
    <td class="tg-yw4l">Minimax-Optimal Privacy-Preserving Sparse PCA in Distributed Systems</td>
    <td class="tg-yw4l">Jason Ge*, Princeton University; Zhaoran Wang, ; Mengdi Wang, ; Han Liu, Princeton</td>
  </tr>
  <tr>
    <td class="tg-yw4l">525</td>
    <td class="tg-yw4l">Online Regression with Partial Information: Generalization and Linear Projection</td>
    <td class="tg-yw4l">Shinji Ito*, NEC Coorporation; Daisuke Hatano, ; Hanna Sumita, ; Akihiro Yabe, ; Takuro Fukunaga, ; Naonori Kakimura, ; Ken-Ichi Kawarabayashi, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">526</td>
    <td class="tg-yw4l">Learning Generative Models with Sinkhorn Divergences</td>
    <td class="tg-yw4l">Aude Genevay*, Université Paris Dauphine; Gabriel Peyre, ; Marco Cuturi, ENSAE/CREST</td>
  </tr>
  <tr>
    <td class="tg-yw4l">532</td>
    <td class="tg-yw4l">Reparameterizing the Birkhoff Polytope for Variational Permutation Inference</td>
    <td class="tg-yw4l">Scott Linderman, ; Gonzalo Mena*, Columbia University; Hal Cooper, Columbia University; Liam Paninski, Columbia University; John Cunningham, Columbia University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">534</td>
    <td class="tg-yw4l">Achieving the time of 1-NN, but the accuracy of k-NN</td>
    <td class="tg-yw4l">Lirong Xue*, Princeton University; Samory Kpotufe, Princeton University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">535</td>
    <td class="tg-yw4l">Efficient Weight Learning in High-Dimensional Untied MLNs</td>
    <td class="tg-yw4l">Khan Mohammad Al Farabi*, The University of Memphis; Somdeb Sarkhel, Adobe Research; Deepak Venugopal, University of Memphis</td>
  </tr>
  <tr>
    <td class="tg-yw4l">536</td>
    <td class="tg-yw4l">Consistent Algorithms for Classification under Complex Losses and Constraints</td>
    <td class="tg-yw4l">Harikrishna Narasimhan*, Harvard University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">539</td>
    <td class="tg-yw4l">Solving lp-norm regularization with tensor kernels</td>
    <td class="tg-yw4l">Saverio Salzo*, Istituto Italiano di Tecnologi; Lorenzo Rosasco, University of Genova &amp; MIT; Johan Suykens, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">546</td>
    <td class="tg-yw4l">Weighted Tensor Decomposition for Learning Latent Variables with Partial Data</td>
    <td class="tg-yw4l">Omer Gottesman*, Harvard University; Weiwei Pan, ; Finale Doshi-Velez, Harvard</td>
  </tr>
  <tr>
    <td class="tg-yw4l">547</td>
    <td class="tg-yw4l">Multi-objective Contextual Bandit Problem with Similarity Information</td>
    <td class="tg-yw4l">Eralp Turgay, Bilkent University; Doruk Oner, Bilkent University; Cem Tekin*, Bilkent University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">549</td>
    <td class="tg-yw4l">Turing: Composable inference for probabilistic programming</td>
    <td class="tg-yw4l">Hong Ge*, University of Cambridge; Kai Xu, University of Edinburgh; Zoubin Ghahramani, University of Cambridge</td>
  </tr>
  <tr>
    <td class="tg-yw4l">550</td>
    <td class="tg-yw4l">Fast and Scalable Learning of Sparse Changes in High-Dimensional Gaussian Graphical Model Structure</td>
    <td class="tg-yw4l">Beilun Wang*, University of Virginia; arshdeep Sekhon, University of Virginia; Yanjun Qi, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">551</td>
    <td class="tg-yw4l">Data-Efficient Reinforcement Learning with \\Probabilistic Model Predictive Control</td>
    <td class="tg-yw4l">Sanket Kamthe, Imperial College; Marc Deisenroth*, Imperial College London</td>
  </tr>
  <tr>
    <td class="tg-yw4l">557</td>
    <td class="tg-yw4l">Approximate Bayesian Computation with Kullback-Leibler Divergence as Data Discrepancy</td>
    <td class="tg-yw4l">Bai Jiang*, Princeton University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">561</td>
    <td class="tg-yw4l">Practical Bayesian optimization in the presence of outliers</td>
    <td class="tg-yw4l">Ruben Martinez-Cantin*, ; Michael McCourt, SigOpt; Kevin Tee, SigOpt</td>
  </tr>
  <tr>
    <td class="tg-yw4l">563</td>
    <td class="tg-yw4l">Competing with Automata-based Expert Sequences</td>
    <td class="tg-yw4l">Scott Yang*, D. E. Shaw &amp; Co.; Mehryar Mohri, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">564</td>
    <td class="tg-yw4l">Reducing Crowdsourcing to Graphon Estimation, Statistically</td>
    <td class="tg-yw4l">Christina Lee*, Microsoft Research; Devavrat Shah, MIT</td>
  </tr>
  <tr>
    <td class="tg-yw4l">567</td>
    <td class="tg-yw4l">Robust Locally-Linear Controllable Embedding</td>
    <td class="tg-yw4l">Ershad Banijamali*, University of Waterloo; Rui Shu, Stanford University; mohammad Ghavamzadeh, DeepMind; Hung Bui, Adobe Research; Ali Ghodsi, University of Waterloo </td>
  </tr>
  <tr>
    <td class="tg-yw4l">569</td>
    <td class="tg-yw4l">Combinatorial Semi-Bandits with Knapsacks</td>
    <td class="tg-yw4l">Karthik Abinav Sankararaman*, University of Maryland College; Aleksandrs Slivkins, Microsoft Research NYC</td>
  </tr>
  <tr>
    <td class="tg-yw4l">571</td>
    <td class="tg-yw4l">Structured Optimal Transport</td>
    <td class="tg-yw4l">David Alvarez Melis*, MIT; Tommi Jaakkola, MIT; Stefanie Jegelka, MIT</td>
  </tr>
  <tr>
    <td class="tg-yw4l">578</td>
    <td class="tg-yw4l">Graphical Models for Non-Negative Data Using Generalized Score Matching</td>
    <td class="tg-yw4l">Shiqing Yu*, University of Washington; Mathias Drton, University of Washington; Ali Shojaie, University of Washington</td>
  </tr>
  <tr>
    <td class="tg-yw4l">581</td>
    <td class="tg-yw4l">Asynchronous Doubly Stochastic Group Regularized Learning</td>
    <td class="tg-yw4l">Bin Gu*, University of Pittsburgh; Zhouyuan Huo, ; Heng Huang, University of Pittsburgh</td>
  </tr>
  <tr>
    <td class="tg-yw4l">582</td>
    <td class="tg-yw4l">Convergence of Value Aggregation for Imitation Learning</td>
    <td class="tg-yw4l">Ching-An Cheng*, Georgia Institute of Technology; Byron Boots, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">594</td>
    <td class="tg-yw4l">Inference in Sparse Graphs with Pairwise Measurements and Side Information</td>
    <td class="tg-yw4l">Dylan Foster*, Cornell University; Karthik Sridharan, Cornell University; Daniel Reichman, UC Berkeley</td>
  </tr>
  <tr>
    <td class="tg-yw4l">595</td>
    <td class="tg-yw4l">Parallel and Distributed MCMC via Shepherding Distributions</td>
    <td class="tg-yw4l">Arkabandhu Chowdhury*, Rice University; Christopher Jermaine, Rice University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">602</td>
    <td class="tg-yw4l">The Power Mean Laplacian for Multilayer Graph Clustering</td>
    <td class="tg-yw4l">Pedro Mercado*, Saarland University; Antoine Gautier, Saarland University; Francesco Tudisco, University of Strathclyde; Matthias  Hein, Saarland University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">604</td>
    <td class="tg-yw4l">Adaptive Sampling for Clustered Ranking</td>
    <td class="tg-yw4l">Sumeet Katariya*, Univ of Wisconsin-Madison; Lalit Jain, University of Michigan Ann Arbor; Nandana Sengupta, University of Chicago; James Evans, University of Chicago; Robert Nowak, University of Wisconsin-Madison</td>
  </tr>
  <tr>
    <td class="tg-yw4l">611</td>
    <td class="tg-yw4l">Comparison Based Learning from Weak Oracles</td>
    <td class="tg-yw4l">Ehsan Kazemi*, Yale; Lin Chen, Yale University; Sanjoy Dasgupta, University of California San Diego; Amin Karbasi, Yale</td>
  </tr>
  <tr>
    <td class="tg-yw4l">613</td>
    <td class="tg-yw4l">The Binary Space Partitioning-Tree Process</td>
    <td class="tg-yw4l">Xuhui Fan*, UNSW; Bin Li, Fudan University; Scott Sisson, University of New South Wales</td>
  </tr>
  <tr>
    <td class="tg-yw4l">614</td>
    <td class="tg-yw4l">On denoising noisy modulo 1 samples of a function</td>
    <td class="tg-yw4l">Mihai  Cucuringu*, University of Oxford and the Alan Turing Institute; Hemant Tyagi, Alan Turing Institute</td>
  </tr>
  <tr>
    <td class="tg-yw4l">616</td>
    <td class="tg-yw4l">Scalable Hash-Based Estimation of Divergence Measures</td>
    <td class="tg-yw4l">Morteza Noshad Iranzad*, University of Michigan; Alfred Hero, University of Michigan</td>
  </tr>
  <tr>
    <td class="tg-yw4l">619</td>
    <td class="tg-yw4l">Conditional Gradient Method for Stochastic Submodular Maximization: Closing the Gap</td>
    <td class="tg-yw4l">Aryan Mokhtari*, UC Berkeley; Hamed Hassani, ; Amin Karbasi, Yale</td>
  </tr>
  <tr>
    <td class="tg-yw4l">620</td>
    <td class="tg-yw4l">Online Continuous Submodular Maximization</td>
    <td class="tg-yw4l">Lin Chen*, Yale University; Hamed Hassani, ; Amin Karbasi, Yale</td>
  </tr>
  <tr>
    <td class="tg-yw4l">626</td>
    <td class="tg-yw4l">Efficient Bayesian Methods for Counting Processes in Partially Observable Environments</td>
    <td class="tg-yw4l">Ferdian Jovan*, University of Birmingham; Jeremy Wyatt, University of Birmingham; Nick Hawes, University of Oxford</td>
  </tr>
  <tr>
    <td class="tg-yw4l">629</td>
    <td class="tg-yw4l">Matrix-normal models for fMRI analysis</td>
    <td class="tg-yw4l">Michael Shvartsman*, Princeton University ; Narayanan Sundaram, Intel Corporation; Mikio Aoi, Princeton University; Adam Charles, Princeton University; Theodore Wilke, Intel Corporation; Jonathan Cohen, Princeton University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">631</td>
    <td class="tg-yw4l">The emergence of spectral universality in deep networks</td>
    <td class="tg-yw4l">Jeffrey Pennington*, ; Samuel Schoenholz, Google; Surya Ganguli, Google Brain</td>
  </tr>
  <tr>
    <td class="tg-yw4l">635</td>
    <td class="tg-yw4l">Spectral Algorithms for Computing Fair Support Vector Machines</td>
    <td class="tg-yw4l">Mahbod Olfat*, UC Berkeley; Anil Aswani, UC Berkeley</td>
  </tr>
  <tr>
    <td class="tg-yw4l">636</td>
    <td class="tg-yw4l">Bayesian Multi-label Learning with Sparse Features and Labels</td>
    <td class="tg-yw4l">He Zhao*, Monash University; Piyush Rai, IIT Kanpur; Lan Du, """Faculty of Information Technology, Monash University, Australia"""; Wray Buntine, Monash University</td>
  </tr>
  <tr>
    <td class="tg-yw4l">637</td>
    <td class="tg-yw4l">Nonparametric Bayesian sparse graph linear dynamical systems</td>
    <td class="tg-yw4l">Rahi Kalantari, UT-Austin; Joydeep Ghosh, UT Austin; Mingyuan Zhou*, University of Texas at Austin</td>
  </tr>
  <tr>
    <td class="tg-yw4l">639</td>
    <td class="tg-yw4l">Proximity Variational Inference</td>
    <td class="tg-yw4l">Jaan Altosaar*, Princeton University; Rajesh Ranganath, Princeton; David Blei, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">641</td>
    <td class="tg-yw4l">Near-Optimal Machine Teaching via Explanatory Teaching Sets</td>
    <td class="tg-yw4l">Yuxin Chen*, Caltech; Oisin  Mac Aodha, Caltech; Shihan Su, Caltech; Pietro Perona, Caltech; Yisong Yue, Caltech</td>
  </tr>
  <tr>
    <td class="tg-yw4l">643</td>
    <td class="tg-yw4l">Learning Hidden Quantum Markov Models</td>
    <td class="tg-yw4l">Siddarth Srinivasan*, Georgia Institute of Technolog; Geoff Gordon, Carnegie Mellon University; Byron Boots, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">644</td>
    <td class="tg-yw4l">Labeled Graph Clustering via Projected Gradient Descent</td>
    <td class="tg-yw4l">Shiau Hong Lim*, IBM Research; Gregory Calvez, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">646</td>
    <td class="tg-yw4l">Gradient Diversity: a Key Ingredient for Scalable Distributed Learning</td>
    <td class="tg-yw4l">Dong Yin*, UC Berkeley; Ashwin Pananjady, UC Berkeley; Max Lam, Stanford University; Dimitris Papailiopoulos, ; Kannan Ramchandran, UC Berkeley; Peter Bartlett, UC Berkeley</td>
  </tr>
  <tr>
    <td class="tg-yw4l">648</td>
    <td class="tg-yw4l">HONES: A Fast and Tuning-free Homotopy Method For Online Newton Step</td>
    <td class="tg-yw4l">Yuting Ye*, UC Berkeley; LIhua Lei, UC Berkeley; Cheng Ju, UC Berkeley</td>
  </tr>
  <tr>
    <td class="tg-yw4l">649</td>
    <td class="tg-yw4l">Probability–Revealing Samples</td>
    <td class="tg-yw4l">Krzysztof Onak*, IBM Research; Xiaorui Sun, Microsoft Research</td>
  </tr>
  <tr>
    <td class="tg-yw4l">656</td>
    <td class="tg-yw4l">Reducing optimization to repeated classification</td>
    <td class="tg-yw4l">Tatsunori Hashimoto*, Stanford; Steve Yadlowsky, Stanford University; John Duchi, </td>
  </tr>
  <tr>
    <td class="tg-yw4l">661</td>
    <td class="tg-yw4l">Online Ensemble Multi-kernel Learning Adaptive to Non-stationary and Adversarial Environments</td>
    <td class="tg-yw4l">Yanning Shen, ; Tianyi Chen*, University of Minnesota; Georgios Giannakis, University of Minnesota</td>
  </tr>
  <tr>
    <td class="tg-yw4l">665</td>
    <td class="tg-yw4l">A Unified Dynamic Approach to Sparse Model Selection</td>
    <td class="tg-yw4l">Chendi Huang*, Peking University; Yuan Yao, Hongkong University of Science and Techonology</td>
  </tr>
  <tr>
    <td class="tg-yw4l">666</td>
    <td class="tg-yw4l">Bootstrapping EM via Power EM and Convergence in the Naive Bayes Model</td>
    <td class="tg-yw4l">Costis Daskalakis, ; Christos Tzamos*, Microsoft Research; Manolis Zampetakis, MIT</td>
  </tr>
  <tr>
    <td class="tg-yw4l">669</td>
    <td class="tg-yw4l">Dimensionality Reduced $\ell^{0}$-Sparse Subspace Clustering</td>
    <td class="tg-yw4l">Yingzhen Yang*, Snap Research</td>
  </tr>
</table></div>
<!--&lt;!&ndash;A Fast and Scalable Joint Estimator for Learning Multiple Related Sparse Gaussian Graphical Models	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Beilun Wang, Ji Gao, Yanjun Qi <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;A Framework for Optimal Matching for Causal Inference <br>&ndash;&gt;-->
<!--&lt;!&ndash;Nathan Kallus <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;A Learning Theory of Ranking Aggregation	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Anna KORBA, Stéphan Clemençon, Eric Sibony <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;A Lower Bound on the Partition Function of Attractive Graphical Models in the Continuous Case<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Nicholas Ruozzi<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;A Maximum Matching Algorithm for Basis Selection in Spectral Learning<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Ariadna Quattoni,  Xavier Carreras, Matthias Gallé<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;A New Class of Private Chi-Square Hypothesis Tests	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Ryan Rogers, Daniel Kifer<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;A Stochastic Nonconvex Splitting Method for Symmetric Nonnegative Matrix Factorization	&ndash;&gt;-->
<!--&lt;!&ndash;<br>Songtao Lu, Mingyi Hong, Zhengdao Wang <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;A Sub-Quadratic Exact Medoid Algorithm	<br>&ndash;&gt;-->
<!--&lt;!&ndash;James Newling, Francois Fleuret <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;A Unified Computational and Statistical Framework for Nonconvex Low-rank Matrix Estimation	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Lingxiao Wang, Xiao Zhang, Quanquan Gu<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;A Unified Optimization View on Generalized Matching Pursuit and Frank-Wolfe	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Francesco Locatello,  Rajiv Khanna, Michael Tschannen, Martin Jaggi<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Active Positive Semidefinite Matrix Completion: Algorithms, Theory and Applications	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Aniruddha  Bhargava, Ravi Ganti, Rob Nowak<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Adaptive ADMM with Spectral Penalty Parameter Selection<br>&ndash;&gt;-->
<!--&lt;!&ndash;Zheng Xu, Mario Figueiredo, Tom Goldstein <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;An Information-Theoretic Route from Generalization in Expectation to Generalization in Probability	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Ibrahim Alabdulmohsin<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Annular Augmentation Sampling<br>&ndash;&gt;-->
<!--&lt;!&ndash;Francois Fagan, Jalaj Bhandari, John Cunningham<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Anomaly Detection in Extreme Regions via Empirical MV-sets on the Sphere<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Albert Thomas, Stéphan Clemençon, Alexandre Gramfort, Anne Sabourin<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;ASAGA: Asynchronous Parallel SAGA	<br>&ndash;&gt;-->
 <!--&lt;!&ndash;Rémi Leblond, Fabian Pedregosa, Simon Lacoste-Julien<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Asymptotically exact inference in likelihood-free models<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Matthew Graham, Amos Storkey<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Attributing Hacks<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Ziqi Liu, Alex Smola, Kyle Soska, Yu-Xiang Wang, Qinghua Zheng <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Automated Inference with Adaptive Batches<br>&ndash;&gt;-->
<!--&lt;!&ndash;Soham De, Abhay Yadav, David Jacobs, Tom Goldstein <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Bayesian Hybrid Matrix Factorisation for Data Integration<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Thomas Brouwer, Pietro Lio<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Belief Propagation in Conditional RBMs for Structured Prediction<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Wei Ping, Alex Ihler <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Meelis Kull, Telmo de Menezes e Silva Filho, Peter Flach&ndash;&gt;-->
 <!--&lt;!&ndash;<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Binary and Multi-Bit Coding for Stable Random Projections<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Ping Li <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Black-box Importance Sampling<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Qiang Liu, Jason Lee<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Clustering from Multiple Uncertain Experts<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Yale Chang, Junxiang Chen,  Michael Cho,  Peter Castaldi, Ed Silverman, Jennifer Dy <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Co-Occuring Directions Sketching for Approximate Matrix Multiply<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Youssef Mroueh, Etienne Marcheret, Vaibahava Goel<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Combinatorial Topic Models using Small-Variance Asymptotics<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Ke Jiang, Suvrit Sra, Brian Kulis<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Communication-efficient Distributed Sparse Linear Discriminant Analysis<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Lu Tian, Quanquan Gu <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Communication-Efficient Learning of Deep Networks from Decentralized Data<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Brendan McMahan, Eider Moore, Daniel Ramage,  Seth Hampson,  Blaise Aguera y Arcas<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Comparison Based Nearest Neighbor Search	<br> &ndash;&gt;-->
<!--&lt;!&ndash;Siavash Haghiri, Ulrike von Luxburg, Debarghya Ghoshdastidar<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Complementary Sum Sampling for Likelihood Approximation in Large Scale Classification<br>	&ndash;&gt;-->
<!--&lt;!&ndash;David Barber, Aleksandar Botev,  Bowen Zheng<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Compressed Least Squares Regression revisited<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Martin Slawski<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Conditions beyond treewidth for tightness of higher-order LP relaxations<br>&ndash;&gt;-->
<!--&lt;!&ndash;Mark Rowland, Aldo Pacchiano,  Adrian Weller<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Conjugate-Computation Variational Inference : Converting Variational Inference in Non-Conjugate Models to Inferences in Conjugate Models<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Mohammad Khan, Wu Lin<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Consistent and Efficient Nonparametric Different-Feature Selection<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Satoshi Hara, Takayuki Katsuki, Hiroki Yanagisawa, Takafumi Ono, Ryo Okamoto, Shigeki Takeuchi<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Contextual Bandits with Latent Confounders: An NMF Approach<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Rajat Sen, Karthikeyan Shanmugam, Murat Kocaoglu, Alex Dimakis, Sanjay Shakkottai<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Convergence rate of stochastic k-means<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Cheng Tang, Claire Monteleoni<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;ConvNets with Smooth Adaptive Activation Functions for Regression<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Le Hou,  Dimitris Samaras,  Tahsin Kurc,  Yi Gao,  Joel Saltz<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;CPSG-MCMC: Clustering-Based Preprocessing method for Stochastic Gradient MCMC<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Tianfan Fu, Zhihua Zhang<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Data Driven Resource Allocation for Distributed Learning<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Travis Dick, Venkata Krishna Pillutla, Mu Li, Colin White, Nina Balcan, Alex Smola<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Decentralized Collaborative Learning of Personalized Models over Networks<br>&ndash;&gt;-->
<!--&lt;!&ndash;Paul Vanhaesebrouck, Aurélien Bellet, Marc Tommasi<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Detecting Dependencies in High-Dimensional, Sparse Databases Using Probabilistic Programming and Non-parametric Bayes<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Feras Saad, Vikash Mansinghka<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Discovering and Exploiting Additive Structure for Bayesian Optimization<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Jacob Gardner, Chuan Guo, Kilian Weinberger, Roman Garnett, Roger Grosse<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Distance Covariance Analysis<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Benjamin Cowley, Joao Semedo,  Amin Zandvakili, Adam Kohn, Matthew Smith, Byron Yu<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Distributed Sequential Sampling for Kernel Matrix Approximation<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Daniele Calandriello, Alessandro Lazric, Michal Valko<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Distribution of Gaussian Process Arc Lengths<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Justin Bewsher, Alessandra Tosi, Michael Osborne, Stephen Roberts<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Diversity Leads to Generalization in Neural Networks<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Bo Xie, Yingyu Liang, Le Song<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;DP-EM: Differentially Private Expectation Maximization<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Mijung Park,  James  Foulds,  Kamalika Choudhary, Max Welling <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Dynamic Collaborative Filtering With Compound Poisson Factorization<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Ghassen Jerfel, Basbug, Barbara Engelhardt <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Efficient Algorithm for Sparse Tensor-variate Gaussian Graphical Models via Gradient Descent<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Pan Xu, Quanquan Gu <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Efficient Multiclass Prediction on Graphs via Surrogate Losses<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Alexander Rakhlin, Karthik Sridharan<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Efficient Rank Aggregation via Lehmer Codes<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Pan Li,  Arya Mazumdar, Olgica Milenkovic<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Encrypted accelerated least squares regression<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Pedro Esperanca, Louis Aslett,  Chris Holmes<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Estimating Density Ridges by Direct Estimation of Density-Derivative-Ratios<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Hiroaki Sasaki, Takafumi Kanamori, Masashi Sugiyama<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Exploration&#45;&#45;Exploitation in MDPs with Options<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Ronan Fruit,  Alessandro Lazric <br><br>&ndash;&gt;-->



<!--&lt;!&ndash;Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Aaron Klein, Stefan Falkner,  Simon Bartels, Philipp Hennig,  Frank Hutter<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Fast Classification with Binary Prototypes<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Kai Zhong,  Ruiqi Guo, Sanjiv Kumar, Bowei Yan, David Simcha, Inderjit Dhillon <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Fast column generation for atomic norm regularization.<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Marina Vinyes, Guillaume Obozinski <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Fast rates with high probability in exp-concave statistical learning<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Nishant Mehta<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Faster Coordinate Descent via Adaptive Importance Sampling<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Dmytro Perekrestenko, Volkan Cevher,  Martin Jaggi<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Finite-sum Composition Optimization via Variance Reduced Gradient Descent<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Xiangru Lian, Ji Liu, Mengdi Wang <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Linking Micro Event History to Macro Prediction in Point Process Models<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Yichen Wang, Xiaojing Ye,  Haomin Zhou, Hongyuan Zha, Le Song<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Frank-Wolfe Algorithms for Saddle Point Problems<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Gauthier Gidel, Simon Lacoste-Julien,  Tony Jebara<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Frequency Domain Predictive Modelling with Aggregated Data<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Avradeep Bhowmik, Joydeep Ghosh, Oluwasanmi Koyejo <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Generalization Error of Invariant Classifiers<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Jure Sokolic, Raja Giryes, Guillermo Sapiro,  Miguel Rodrigues<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Generalized Pseudolikelihood Methods for Inverse Covariance Estimation<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Alnur Ali, Kshitij Khare, Sang-Yun Oh, Bala Rajaratnam <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Global Convergence of Non-Convex Gradient Descent for Computing Matrix Squareroot<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Prateek Jain, Chi Jin, Sham Kakade, Praneeth Netrapalli <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Gradient Boosting on Stochastic Data Streams<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Hanzhang Hu, Andrew Bagnell,  Wen Sun,  Martial Hebert, Arun Venkatraman<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Gray-box inference for structured Gaussian process models<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Pietro Galliani, Amir Dezfouli,  Edwin Bonilla,  Novi Quadrianto<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Greedy Direction Method of Multiplier for MAP Inference of Large Output Domain<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Xiangru Huang, Ian En-Hsu Yen, Ruohan Zhang, Qixing Huang, Pradeep Ravikumar, Inderjit  Dhillon<br><br>&ndash;&gt;-->

	<!--&lt;!&ndash;&ndash;&gt;-->
<!--&lt;!&ndash;Guaranteed Non-convex Optimization: Submodular Maximization over Continuous Domains<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Andrew An Bian, Baharan Mirzasoleiman, Joachim Buhmann, Andreas Krause<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Hierarchically-partitioned Gaussian Process Approximation<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Byung-Jun Lee, Jongmin Lee, Kee-Eung Kim<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;High-dimensional Time Series Clustering via Cross-Predictability<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Dezhi Hong, Quanquan Gu, Kamin Whitehouse<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Hit-and-Run for Sampling and Planning in Non-Convex Spaces<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Yasin  Abbasi-Yadkori, Alan Malek, Peter Bartlett, Victor Gabillon<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Horde of Bandits using Gaussian Markov Random Fields<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Sharan Vaswani, Mark Schmidt,  Laks Lakshmanan<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Identifying groups of strongly correlated variables through Smoothed Ordered Weighted L_1-norms<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Raman Sankaran,  Francis  Bach, Chiranjib Bhattacharya <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Improved Strongly Adaptive Online Learning using Coin Betting<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Kwang-Sung Jun, Rebecca Willett, Stephen Wright, Francesco Orabona<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Inference Compilation and Universal Probabilistic Programming<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Tuan Anh Le, Atilim Gunes Baydin, Frank Wood <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Information Projection and Approximate Inference for Structured Sparse Variables<br>&ndash;&gt;-->
	 <!--&lt;!&ndash;Rajiv Khanna, Joydeep Ghosh, Rusell Poldrack, Oluwasanmi Koyejo <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Information-theoretic limits of Bayesian network structure learning<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Asish Ghoshal, Jean Honorio<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Initialization and Coordinate Optimization for Multi-way Matching<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Da Tang, Tony Jebara<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Label Filters for Large Scale Multilabel Classification<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Alexandru Niculescu-Mizil, Ehsan Abbasnejad <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Large-Scale Data-Dependent Kernel Approximation<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Alin Popa, Catalin Ionescu, Cristian Sminchisescu <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Learning Cost-Effective Treatment Regimes using Markov Decision Processes<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Himabindu Lakkaraju, Cynthia Rudin<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Learning from Conditional Distributions via Dual Kernel Embeddings<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Bo Dai, Niao He, Yunpeng Pan, Byron Boots, Le Song<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Learning Graphical Games from Behavioral Data: Sufficient and Necessary Conditions<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Asish Ghoshal,  Jean Honorio<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Learning Nash Equilibrium for General-Sum Markov Games from Batch Data<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Julien Perolat, Florian Strub, Bilal Piot,  Olivier Pietquin<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Learning Nonparametric Forest Graphical Models with Prior Information<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Yuancheng Zhu, Zhe Liu,  Siqi Sun<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Learning Optimal Interventions<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Jonas Mueller,  David  Reshef, George Du,  Tommi Jaakkola<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Learning Structured Weight Uncertainty in Bayesian Neural Networks<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Shengyang Sun, Changyou Chen, Lawrence Carin<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Learning the Network Structure of Heterogeneous Data via Pairwise Exponential Markov Random Fields	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Youngsuk Park, David Hallac, Stephen Boyd,  Jure Leskovec<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Learning Theory for Conditional Risk Minimization<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Alexander Zimin, Christoph Lampert <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Learning Time Series Detection Models from Temporally Imprecise Labels<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Roy Adams, Ben Marlin<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Learning with feature feedback: from theory to practice	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Stefanos Poulis, Sanjoy Dasgupta<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Least-Squares Log-Density Gradient Clustering for Riemannian Manifolds<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Mina Ashizawa, Hiroaki Sasaki, Tomoya Sakai,  Masashi Sugiyama<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Less than a Single Pass: Stochastically Controlled Stochastic Gradient Method<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Lihua Lei,  Michael Jordan<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Linear Convergence of Stochastic Frank Wolfe Variants<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Chaoxu Zhou, Donald Goldfarb, Garud Iyengar<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Linear Thompson Sampling Revisited<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Marc Abeille, Alessandro Lazric <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Lipschitz Density-Ratios, Structured Data, and Data-driven Tuning<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Samory Kpotufe<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Local Group Invariant Representations via Orbit Embeddings<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Anant Raj,  Abhishek  Kumar, Youssef Mroueh, Tom Fletcher, Bernhard Schoelkopf<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Local Perturb-and-MAP for Structured Prediction<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Gedas Bertasius,  Lorenzo Torresani, Jianbo Shi, Qiang Liu <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Localized Lasso for High-Dimensional Regression<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Makoto Yamada,  Takeuchi Koh,  Tomoharu Iwata, John Shawe-Taylor, Samuel Kaski <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Lower Bounds on Active Learning for Graphical Model Selection<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Jonathan Scarlett,  Volkan Cevher<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Markov Chain Truncation for Doubly-Intractable Inference<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Colin  Wei,  Iain Murray<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Minimax Approach to Variable Fidelity Data Interpolation<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Alexey Zaytsev, Evgeny Burnaev <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Minimax density estimation for growing dimension<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Daniel McDonald<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Minimax Gaussian Classification & Clustering<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Tianyang  Li,  Xinyang Yi,  Constantine Carmanis,  Pradeep Ravikumar  <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Minimax-optimal semi-supervised regression on unknown manifolds<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Amit Moscovich, Ariel Jaffe, Boaz Nadler  <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Modal-set estimation with an application to clustering<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Heinrich Jiang, Samory Kpotufe<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Near-optimal Bayesian Active Learning with Correlated and Noisy Tests<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Yuxin Chen, Hamed Hassani, Andreas Krause<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Lijie Chen, Jian Li,  Mingda Qiao<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Non-Count Symmetries in Boolean & Multi-Valued Probabilistic Graphical Models<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Parag Singla, Ritesh Noothigattu, Ankit Anand,  Mausam<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Dohyung Park,  Anastasios Kyrillidis, Constantine Carmanis,  Sujay Sanghavi<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Nonlinear ICA of Temporally Dependent Stationary Sources<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Aapo Hyvarinen, Hiroshi Morioka<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;On the Hyperprior Choice for the Global Shrinkage Parameter in the Horseshoe Prior<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Juho Piironen, Aki Vehtari <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;On the Interpretability of Conditional Probability Estimates in the Agnostic Setting<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Yihan Gao,  Aditya Parameswaran,  Jian Peng<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;On the learnability of fully-connected neural networks<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Yuchen Zhang,  Jason Lee,  Martin Wainwright,  Michael Jordan<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;On the Troll-Trust Model for Edge Sign Prediction in Social Networks<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Géraud Le Falher, Nicolo Cesa-Bianchi,  Claudio Gentile, Fabio Vitale <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Online Learning with Partial Monitoring: Optimal Convergence Rates<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Joon Kwon, Vianney Perchet <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Online Nonnegative Matrix Factorization with General Divergences<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Renbo Zhao, Vincent  Tan, Huan Xu<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Online Optimization of Smoothed Piecewise Constant Functions<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Vincent Cohen-Addad,  Varun Kanade <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Optimal Recovery of Tensor Slices<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Andrew Li, Vivek Farias<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Optimistic Planning for the Stochastic Knapsack Problem<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Ciara Pike-Burke, Steffen Grunewalder<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Orthogonal Tensor Decompositions via Two-Mode Higher-Order SVD (HOSVD)<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Miaoyan Wang, Yun Song<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Performance Bounds for Graphical Record Linkage<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Rebecca C. Steorts, Mattew Barnes, Willie Neiswanger<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Phase Retrieval Meets Statistical Learning Theory: A Flexible Convex Relaxation<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Sohail Bahmani, Justin Romberg<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Poisson intensity estimation with reproducing kernels<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Seth Flaxman, Yee Whye Teh,  Dino Sejdinovic<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Prediction Performance After Learning in Gaussian Process Regression<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Johan Wagberg, Dave Zachariah, Thomas Schon,  Petre Stoica <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Quantifying the accuracy of approximate diffusions and Markov chains<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Jonathan Huggins, James Zou<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Random Consensus Robust PCA<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Daniel Pimentel-Alarcon, Robert Nowak<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Random projection design for scalable implicit smoothing of randomly observed stochastic processes	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Francois Belletti,  Evan Sparks,  Alexandre Bayen,  Kurt Keutzer, Joseph Gonzalez<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Rank Aggregation and Prediction with Item Features<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Kai-Yang Chiang, Cho-Jui Hsieh, Inderjit  Dhillon <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Rapid Mixing Swendsen-Wang Sampler for Stochastic Partitioned Attractive Models	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Sejun Park, Yunhun Jang, Andreas Galanis, Jinwoo Shin, Daniel Stefankovic, Eric Vigoda <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Recurrent Switching Linear Dynamical Systems<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Scott Linderman,  Andrew Miller,  David Blei,  Ryan Adams,  Liam Paninski, Matthew Johnson <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Regression Uncertainty on the Grassmannian	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Yi Hong, Xiao Yang, Roland Kwitt,  Martin Styner, Marc Niethammer <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Regret Bounds for Lifelong Learning<br>	&ndash;&gt;-->
<!--&lt;!&ndash;Pierre Alquier, Tien Mai,  Massimiliano Pontil<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Regret Bounds for Transfer Learning in Bayesian Optimisation<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Alistair Shilton, Sunil Gupta, Santu Rana, Svetha Venkatesh<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Rejection Sampling Variational Inference	<br>&ndash;&gt;-->
<!--&lt;!&ndash;Christian Naesseth, Francisco Ruiz, Scott Linderman, David Blei<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Relativistic Monte Carlo <br>&ndash;&gt;-->
	<!--&lt;!&ndash;Xiaoyu Lu, Valerio Perrone, Leonard Hasenclever, Yee Whye Teh, Sebastian Vollmer<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Removing Phase Transitions from Gibbs Measures<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Ian Fellows<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Robust and Efficient Computation of Eigenvectors in a Generalized Spectral Method for Constrained Clustering<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Chengming Jiang, Huiqing Xie, Zhaojun Bai<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Robust Causal Estimation in the Large-Sample Limit without Strict Faithfulness<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Ioan Gabriel Bucur, Tom Heskes, Tom Claassen<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Scalable Convex Multiple Sequence Alignment via Entropy-Regularized Dual Decomposition<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Jiong Zhang, Ian En-Hsu Yen, Pradeep Ravikumar, Inderjit  Dhillon <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Scalable Greedy Support Selection via Weak Submodularity<br>&ndash;&gt;-->
	 <!--&lt;!&ndash;Rajiv Khanna, Ethan Elenberg, Joydeep Ghosh, Alex Dimakis<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Scalable Learning of Non-Decomposable Objectives<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Elad Eban, Mariano Schain, Alan Mackey, Ariel Gordon,  Ryan Rifkin,  Gal Elidan<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Scalable variational inference for super resolution microscopy<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Ruoxi Sun, Evan  Archer,  Liam  Paninski<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Scaling Submodular Maximization via Pruned Submodularity Graphs<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Tianyi Zhou, Hua Ouyang,  Yi Chang,  Jeff Blimes,  Carlos Guestrin<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Sequential Graph Matching with Sequential Monte Carlo<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Seong-Hwan Jun, Alexandre Bouchard-Cote, Samuel W.K. Wong<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Sequential Multiple Hypothesis Testing with Type I Error Control<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Alan Malek, Yinlam Chow, Mohammad Ghavamzadeh, Sumeet Katariya <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Signal-based Bayesian Seismic Monitoring<br>&ndash;&gt;-->
	<!--&lt;!&ndash;David Moore, Stuart Russell<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Sketching Meets Random Projection in the Dual: A Provable Recovery Algorithm for Big and High-dimensional Data<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Jialei Wang, Jason Lee,  Mehrdad Mahdavi,  Mladen Kolar, Nati Srebro<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Sketchy Decisions: Convex Low-Rank Matrix Optimization with Optimal Storage<br>&ndash;&gt;-->
<!--&lt;!&ndash;Alp Yurtsever, Madeleine Udell, Joel Tropp, Volkan Cevher<br><br>&ndash;&gt;-->



<!--&lt;!&ndash;Sparse Accelerated Exponential Weights<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Pierre Gaillard,  Olivier Wintenberger<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Sparse Randomized Partition Trees for Nearest Neighbor Search<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Kaushik Sinha, Omid Keivani<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Spatial Decompositions for Large Scale SVMs<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Philipp Thomann, Ingo Steinwart, Ingrid Blaschzyk,  Mona Meister<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Spectral Methods for Correlated Topic Models<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Forough Arabshahi, Anima Anandkumar<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Stochastic Difference of Convex Algorithm and its Application to Training Deep Boltzmann Machines<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Atsushi Nitanda, Taiji Suzuki<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Stochastic Rank-1 Bandits<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, Claire Vernade, Zheng Wen<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Structured adaptive and random spinners for fast machine learning computations<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Francois Fagan, Cedric Gouy-Pailler, Anne Morvan,  Nourhan Sakr,  Tamas Sarlos,  Jamal Atif <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Tensor-Dictionary Learning with Deep Kruskal-Factor Analysis<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Andrew Stevens, Yunchen Pu, Yannan Sun,  Gregory Spell, Lawrence Carin <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear Bandits<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Tor  Lattimore,  Csaba Szepesvari<br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Thompson Sampling for Linear-Quadratic Control Problems<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Marc Abeille, Alessandro Lazric <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Tracking Objects with Higher Order Interactions via Delayed Column Generation<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Shaofei Wang, Steffen Wolf, Charless Fowlkes, Julian Yarkony<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Trading off Rewards and Errors in Multi-Armed Bandits<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Akram Erraqabi, Alessandro Lazric, Michal Valko, Yun-En Liu,  Emma Brunskill <br><br>&ndash;&gt;-->

<!--&lt;!&ndash;Training Fair Classifiers<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, Krishna Gummadi<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Unsupervised Sequential Sensor Acquisition<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Manjesh Hanawal, Venkatesh Saligrama, Csaba Szepesvari<br><br>&ndash;&gt;-->


<!--&lt;!&ndash;Value-Aware Loss Function for Model-based Reinforcement Learning<br>&ndash;&gt;-->
	<!--&lt;!&ndash;Amir-Massoud Farahmand, Andre Barreto, Daniel Nikovski <br><br>&ndash;&gt;-->


<!--&lt;!&ndash;<hr>&ndash;&gt;-->


<!--&lt;!&ndash;<h1>Reviewer Instructions for AISTATS 2017</h1>&ndash;&gt;-->

<!--&lt;!&ndash;Reviews must be entered electronically through the &ndash;&gt;-->
<!--&lt;!&ndash;<a href="https://cmt.research.microsoft.com/AISTATS2017/">CMT system for AISTATS 2017</a>.&ndash;&gt;-->


<!--&lt;!&ndash;<h3>Review Content</h3>&ndash;&gt;-->
<!--&lt;!&ndash;<p>&ndash;&gt;-->
<!--&lt;!&ndash;Each review should begin with a paragraph providing an overview of the paper, and summarizing its main contributions. In particular, some thought should be given to how the paper fits with the aims and topics of the conference (not interpreted overly narrowly). The paragraph should relate the ideas in the paper to to previous work in the field.&ndash;&gt;-->
<!--&lt;!&ndash;</p>&ndash;&gt;-->
<!--&lt;!&ndash;<p>&ndash;&gt;-->
<!--&lt;!&ndash;The next section of the review should deal with major comments, issues that the reviewer sees as standing in the way of acceptance of the paper, or issues that should be addressed prior to publication, or reasons for rejecting the paper.&ndash;&gt;-->
<!--&lt;!&ndash;</p>&ndash;&gt;-->
<!--&lt;!&ndash;<p>&ndash;&gt;-->
<!--&lt;!&ndash;The final section of the review should deal with any minor issues, such as typographical errors, spelling mistakes, or areas where presentation could be improved.&ndash;&gt;-->
<!--&lt;!&ndash;</p>&ndash;&gt;-->
<!--&lt;!&ndash;<p>&ndash;&gt;-->
<!--&lt;!&ndash;As was done last year, reviewers may request public or non-proprietary code/data as part of the initial reviews for the purpose of better judging the paper.  The authors will then provide the code/data as part of the author response.  This might be, for instance, to check whether the authors' methods work as claimed, or whether it correctly treats particular scenarios the authors did not consider in their initial submission.  Note this request is NOT to be used to ask the authors to release their code after the paper has been published.  Code/data should only be requested in the event that this is the deciding factor in paper acceptance. The request should be reasonable in light of the duration of the discussion period, which limits the time available for review.  The SPC member in charge of the paper will confirm whether a code/data request is warranted and reasonable. Authors may only submit separate code and data at the invitation of a reviewer; otherwise, the usual restrictions apply on author response length. The conference chairs will enable the anonymous transfer of code and data to the relevant reviewers.&ndash;&gt;-->
<!--&lt;!&ndash;</p>&ndash;&gt;-->

<!--<h3>Evaluation Criteria</h3>-->
<!--<p>-->
<!--Contributions of AISTATS papers can be categorized into four areas a) algorithmic, b) theoretical, c) unifying or d) application.-->
<!--</p>-->
<!--<p>-->
<!--Algorithmic contributions may make a particular approach feasible for the first time or may extend the applicability of an approach (for example allowing it to be applied to very large data sets).-->
<!--</p>-->
<!--<p>-->
<!--A theoretical contribution should provide a new result about a model or algorithm. For example convergence proofs, consistency proofs or performance guarantees.-->
<!--</p>-->
<!--<p>-->
<!--A unifying contribution may bring together several apparently different ideas and show how they are related, providing new insights and directions for future research.-->
<!--</p>-->
<!--<p>-->
<!--Finally, an application contribution should typically have aspects that present particular statistical challenges which require solution in a novel way or through clever adaptation of existing techniques.-->
<!--</p>-->
<!--<p>-->
<!--A paper may exhibit one or more of these contributions, where each of them are important in advancing the state of the art in the field. Of course, at AISTATS we are also particularly keen to see work which relates machine learning and statistics or highlights novel connections between the fields or even contrasts them.-->
<!--</p>-->
<!--<p>-->
<!--One aspect of result presentation that is often neglected is a discussion of the failure cases of an algorithm, often due to concern that reviewers will penalize authors who provide this information. We emphasize that description of failure cases as well as successes should be encouraged and rewarded in submissions.-->
<!--</p>-->
<!--<p>-->
<!--When reviewing, bear in mind that one of the most important aspects of a successful conference paper is that it should be thought provoking. Thought provoking papers sometimes generate strong reactions on initial reading, which may sometimes be negative. However, if the paper genuinely represents a paradigm shift it may take a little longer than a regular paper to come around to the author's way of thinking. Keep an eye out for such papers, although they may take longer to review, if they do represent an important advance the effort will be well worth it.-->
<!--</p>-->
<!--<p>-->
<!--Finally, we would like to signal to newcomers to AISTATS (and to machine-learning conferences generally) that the review process is envisioned in exactly the same spirit as in a top quality journal like JRSS B, JASA, or Annals of Statistics. Accepted contributions are published in proceedings, and acceptance is competitive, so authors can rightly include these contributions in their publication list, on par with papers published in top quality journals. Further, AISTATS does not give the option to revise and resubmit: if a paper cannot be accepted with minor revisions (e.g., as proposed by the authors in their response to the reviews), it should be rejected.-->
<!--</p>-->
<!--<p>-->
<!--Given the culture gap between the statistics and machine learning communities, we thus want to emphasize from the start the required levels of quality and innovation. All deadlines are very strict, as we cannot delay an overall tight schedule.-->
<!--</p>-->

<!--<h3>Confidentiality and Double Blind Process</h3>-->

<!--<p>-->
<!--AISTATS 2017 is a double blind reviewed conference. Whilst we expect authors to remove information that will obviously reveal their identity, we also trust reviewers not to take positive steps to try and uncover the authors' identity.-->
<!--</p>-->
<!--<p>-->
<!--We are happy for authors to submit material that they have placed online as tech reports (such as in arXiv), or that they have submitted to existing workshops that do not produce published proceedings. This can clearly present a problem with regard to anonymization. Please do not seek out such reports on line in an effort to deanonymize.-->
<!--</p>-->
<!--<p>-->
<!--The review process is double blind. Authors do not know reviewer identities, and this includes any authors on the senior program committee (i.e., the area chairs). However, area chairs do see reviewer identities. Also, during the discussion phase reviewer identities will be made available to other reviewers. In other words, whilst the authors will not know your identity, your co-reviewers will. This should help facilitate discussion of the papers.-->
<!--</p>-->
<!--<p>-->
<!--If a reviewer requests code from the authors, this code should be anonymized (e.g., author names should be removed from the file headers). That said, we understand that it might be difficult to remove all traces of the authors from the files, and will exercise reasonable judgment if innocent mistakes are made.-->
<!--</p>-->
<!--<p>-->
<!--The AISTATS reviewing process is confidential. By agreeing to review you agree not to use ideas, results, code, and data from submitted papers in your work. This includes research and grant proposals. This applies unless that work has appeared in other publicly available formats, for example technical reports or other published work. You also agree not to distribute submitted papers, ideas, code, or data to anyone else. If you request code and accompanying data, you agree that this is provided for your sole use, and only for the purposes of assessing the submission. All code and data must be discarded once the review is complete, and may not be used in further research or transferred to third parties.-->
<!--</p>-->

<!--<h3>The CMT Reviewing System</h3>-->

<!--<p>The first step in the review process is to enter conflicts of interests. These conflicts can be entered as domain names (e.g., cmu.edu) and also by marking specific authors with whom you have a conflict. The use of double blind reviewing means you may not able to determine the papers you have a conflict with, so it is important you go through this list carefully and mark any conflicts. You should mark a conflict with anyone who is or ever was your student or mentor, is a current or recent colleague, or is a close collaborator. If in doubt, it's probably better to mark a conflict, in order to avoid the appearance of impropriety. Your own username should be automatically marked as a conflict, but sometimes the same person may have more than one account, in which case you should definitely mark your other accounts as a conflict as well. If you do not mark a conflict with an author, it is assumed that you do not have a conflict by default.-->
<!--</p>-->
<!--<p>-->
<!--CMT also requests subject information which will be used to assist allocation of reviewers to papers. Please enter relevant keywords to assist in paper allocation.-->
<!--</p>-->
<!--<p>-->
<!--You can revise your review multiple times before the submission. Your formal invite to be a reviewer will come from the CMT system. The email address used in this invite is your login, you can change your password with a password reset from the login screen.-->
<!--</p>-->

<!--<h3>Supplementary Material</h3>-->
<!--<p>-->
<!--Supplementary material is allowed by AISTATS 2018. For example, this supplementary material could include proofs, video, source code or audio. As a reviewer you should feel free to make use of this supplementary material to help in your review, though reviewing supplementary material is up to your discretion.  One exception is the letter of revision from papers previously submitted to NIPS.  If this letter is present in the supplementary material, we ask you to take it into consideration.-->
<!--</p>-->

<!--<h3>Simultaneous Submission</h3>-->
<!--<p>-->
<!--Simultaneous submission to other conference venues in the areas of machine learning and statistics is not permitted.-->
<!--</p>-->
<!--<p>-->
<!--Simultaneous submission to journal publications of significantly extended versions of the paper is permitted, as long as the publication date of the journal is not before May 2017.-->
<!--</p>-->



<br><br>

